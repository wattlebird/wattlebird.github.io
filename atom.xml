<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dedication</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wattlebird.github.io/"/>
  <updated>2017-10-06T14:58:00.012Z</updated>
  <id>https://wattlebird.github.io/</id>
  
  <author>
    <name>Yet another tech blog!</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mangaki data challenge 1st place solution</title>
    <link href="https://wattlebird.github.io/2017/10/02/Mangaki-data-challenge-1st-place-solution/"/>
    <id>https://wattlebird.github.io/2017/10/02/Mangaki-data-challenge-1st-place-solution/</id>
    <published>2017-10-02T05:52:47.000Z</published>
    <updated>2017-10-06T14:58:00.012Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://universityofbigdata.net/competition/5085548788056064?lang=en" target="_blank" rel="external">Mangaki data challenge</a> is an otaku-flavor oriented data science competition. It’s goal is to predict user’s preference of an unwatched/unread anime/manga from two choices: wish to watch/read and don’t want to watch/read. This competition provides training data from <a href="https://mangaki.fr/" target="_blank" rel="external">https://mangaki.fr/</a> which allows users to favorite their anime/manga works. Three major training tables are provided as described as follows:</p><ol><li>Wish table: about 10k rows</li></ol><table><thead><tr><th>User_id</th><th>Work_id</th><th>Wish</th></tr></thead><tbody><tr><td>0</td><td>233</td><td>1</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><ol><li>Record table: for already watched/read anime/manga. There are four rates here: love, like, neutral and dislike.</li></ol><table><thead><tr><th>User_id</th><th>Work_id</th><th>Rate</th></tr></thead><tbody><tr><td>0</td><td>22</td><td>like</td></tr><tr><td>2</td><td>33</td><td>dislike</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><ol><li>Work table: detailed information of available anime/manga. There are three categories: anime, manga and album. There is only one album in this table, all the others are anime (about 7k) and manga (about 2k)</li></ol><table><thead><tr><th>Work_id</th><th>Title</th><th>Category</th></tr></thead><tbody><tr><td>0</td><td>Some_anime</td><td>anime</td></tr><tr><td>1</td><td>Some_manga</td><td>manga</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>For the testing data, one should predict 100k user/work pair on whether the user wish or not wish to watch/read an anime/manga. As you can see, the testing data is much larger than training data. Besides, during my analysis of this dataset, it is also not ensured that all users or works appeared in test set are contained in training set. </p><h2 id="Traditional-recommendation-system-methods-that-I-know"><a href="#Traditional-recommendation-system-methods-that-I-know" class="headerlink" title="Traditional recommendation system methods (that I know)"></a>Traditional recommendation system methods (that I know)</h2><p>Recommendation system building has long been studied and there are various methods in solving this particular problem. For me, I also tried to build a recommender for <a href="https://bgm.tv" target="_blank" rel="external">https://bgm.tv</a> several years ago (you can read technical details <a href="https://wattlebird.github.io/2015/03/08/How-I-build-up-Chi-%E5%90%8C%E6%AD%A5%E7%8E%87%E6%94%B9-v0-3-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/">here</a>). The simplest solution is SVD (actually, a more simple and intuitive solution is by using KNN), then one can move on to RBM, FM, FFM and so on. One assumption that holds firm in all these methods is that users should have an embedding vector capturing their preferences, and works should also have their embedding vector capturing their characteristics. It is reasonable that we should be constrained in this embedding-dotproduct model?</p><p>Recently, the common practice on Kaggle competition is by using GBDT to solve (almost all except computer vision related) questions. As long as a model can handle classification, regression and ranking problem very well, it can be applied in all supervised machine learning problems! And by using model ensembing under stacknet framework, one can join different characteristics of models altogether to achieve the best result.</p><p>In this competition, my solution is quite fair and straightforward: feature engineering to generate some embeddings, and use GBDT/Random Forest/Factorization Machine to build models from different combinations of features. After all, I used a two-level stack net to ensemble them, in which level two is a logistic regression model.</p><h2 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h2><h3 id="From-wish-table"><a href="#From-wish-table" class="headerlink" title="From wish table:"></a>From wish table:</h3><ul><li>Distribution of user’s preference on anime/manga (2d+2d)</li><li>Distribution of item’s preference (2d)</li><li>Word2vec embedding of user on wish-to-watch items (20d)</li><li>Word2vec embedding of user on not-wish-to-watch items (10d)</li><li>Word2vec embedding of item on wish-to-watch users (20d)</li><li>Word2vec embedding of item on not-wish-to-watch users (10d)</li><li>Lsi embedding of user (20d)</li><li>Lsi embedding of item (20d)</li></ul><h3 id="From-record-table"><a href="#From-record-table" class="headerlink" title="From record table:"></a>From record table:</h3><ul><li>Distribution of user’s preference on anime/manga (4d+4d)</li><li>Distribution of item’s preference (4d)</li><li>Mean/StdErr of user’s rating (2d)</li><li>Mean/StdErr of item’s rating (2d)</li><li>Word2vec embedding of user on loved and liked items (32d)</li><li>Word2vec embedding of user on disliked items (10d)</li><li>Word2vec embedding of item on loved and liked users (32d)</li><li>Word2vec embedding of item on disliked users (10d)</li><li>Lsi embedding of user (20d)</li><li>Lsi embedding of item (20d)</li><li>Lda topic distribution of user on love, like and neutral items (20d)</li><li>Lda topic distribution of item on love, like and neutral ratings (20d)</li><li>Item categorial (1d, categorial feature)</li><li>User Id (1d, only used in FM)</li><li>Item Id (1d, only used in FM)</li></ul><h2 id="Model-ensembing"><a href="#Model-ensembing" class="headerlink" title="Model ensembing"></a>Model ensembing</h2><p>The first layer of stack net is a set of models that should have good capability of prediction but with different inductive bias. Here I just tried three models: GBDT, RF (all backended by <a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="external">lightGBM</a>) and FM (backended by <a href="https://github.com/ibayer/fastFM" target="_blank" rel="external">FastFM</a>). I trained models from record table feature and training table feature separately, and one can further train different models using different combinations of features. For example, one can use all features (except user id and item id) in record table feature. But since GBDT would keep eye on most informative feature if all feature were given, it would be helpful to split features into several groups to train model separately. In this competition, I did not split too much (just because I don’t have too much time). I just removed the first four features (because I see from the prediction result that they have having a major effect on precision) and trained some other models.</p><h2 id="Model-stacking"><a href="#Model-stacking" class="headerlink" title="Model stacking"></a>Model stacking</h2><p>The stack net requires one to feed all prediction result from the first layer as feature to second feature. The stacking technique requires one to do KFold cross-validation at the beginning, and then to predict each fold’s result based on all other folds as training data on the second level. Here is the most intuitive (as far as I think) description of model stacking technique: <a href="http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/" target="_blank" rel="external">http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/</a></p><p>In this competition, by using a single GBDT and all the features from record table one can reach 0.85567 on LB. By leveraging model stacking technique, one can reach to 0.86155, which is my final score.</p><h2 id="Is-this-the-ultimate-ceiling"><a href="#Is-this-the-ultimate-ceiling" class="headerlink" title="Is this the ultimate ceiling?"></a>Is this the ultimate ceiling?</h2><p>Definitely not. One can push the boundary much further:</p><ol><li>I did not tune the embedding generation parameters very well. In fact, I generated those features using default parameters <a href="https://radimrehurek.com/gensim/" target="_blank" rel="external">gensim</a> provided. The dimension of embeddings are just get by my abrupt decision, no science involved. Maybe one can enlarge the sliding window of word2vec or use more embedding dimensions to achieve better results.</li><li>I only used lightGBM to build GBDT. One can also use xgboost. Even though they all provides GBDT, lightGBM is <a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md" target="_blank" rel="external">a leaf-wise tree growth algorithm based model, while xgboost is depth-wise tree growth</a>. Even though two models are all CART based GBDT, they behaves differently.</li><li>I did not introduced any deep model generated features. GBDT is such a kind of model that relies on heavy feature engineering while deep model would learn features automatically. By combining them altogether in stacking model one can obtain much higher AUC definitely.</li><li>I did not use more complex features. Sometimes, population raking would also effect user’s behavior. A user would select those animes ranked high as “wish to watch”. I did not tried this idea out.</li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I must say this competition is very interesting because I see no other competition targets on anime/manga prediction. Another good point of this competition is that the training data is very small, so that I could do CV efficiently on my single workstation. And before this competition, I have never tried stack net before. This competition granted me some experience in how to do model stacking in an engineering experience friendly way.</p><p>One thing to regret is that too few competitors were involved in this competition. Though I tried to call for participants to join on <a href="http://chii.in/group/topic/343808" target="_blank" rel="external">Bangumi</a>, it seems still not many people joined. The competition holder should make their website more popular next time before holding next data challenge!</p><p>One more thing: one may be interested in the code. I write all my code <a href="https://github.com/wattlebird/MangakiChallenge" target="_blank" rel="external">here</a> but they are not arranged in an organized way. But I think the most important files are: “FeatureExtraction.ipynb” and “aggregation.py”. They are files about how to do feature engineering and how to partition features. “CV.ipynb” gives some intuition on how to train models.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://universityofbigdata.net/competition/5085548788056064?lang=en&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Mangaki data challenge&lt;/a&gt; i
      
    
    </summary>
    
      <category term="Competition" scheme="https://wattlebird.github.io/categories/Competition/"/>
    
    
      <category term="Machine Learning" scheme="https://wattlebird.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Console as a SQL interface for quick text file processing</title>
    <link href="https://wattlebird.github.io/2017/04/14/Console-as-a-SQL-interface-for-quick-text-file-processing/"/>
    <id>https://wattlebird.github.io/2017/04/14/Console-as-a-SQL-interface-for-quick-text-file-processing/</id>
    <published>2017-04-14T15:18:56.000Z</published>
    <updated>2017-04-16T11:50:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在处理业务上的某些事情时，会发现这样的问题:为了调试某个程序，会 dump 出一堆中间文件的 log，去查找哪些地方发生了异常。这种文件都是 tsv 文件，即使用 TAB 分割的列表文件。一种最简单的做法就是在文本编辑器中打开这些文件，然后通过观察去查看异常，可以配合编辑器的搜索功能。在这方面 sublime text 最为适合，因为只有这个能打开大文件。但是，这样每一次打开都需要很长的时间，而且我去搜索某个字符串的时候，每输入一个字符编辑器就会停顿很长时间。</p><p>于是我希望能有这么一种东西，对文本文件这种没有被某种结构化工具存储的、没有明确定义 schema 的东西进行快速的查找、计算。简单地借用 SQL 中的术语，我希望能在不导入数据的情况下进行 <code>SELECT</code>、 <code>WHERE</code>、 <code>ORDER</code>、 <code>LIMIT</code>、 <code>JOIN</code> 等基本功能。幸运的是，最近发现了以前打印出来一直都没看的讲义，一些最基本的 Unix command 就基本上涵盖了我所希望的对文本文件处理的功能。本文就按照这些 SQL 功能语句把这些命令进行梳理。</p><p>本文进行处理的数据对象是在 2 月用 <a href="https://github.com/wattlebird/Bangumi_Spider" target="_blank" rel="external">Bangumi_Spider</a> 爬取的 Bangumi Data，这包括用户、条目和收藏记录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">head user-2017-02-17T12_26_12-2017-02-19T06_06_44.tsv -n 5</div><div class="line">head record-2017-02-20T14_03_27-2017-02-24T10_57_16.tsv -n 5</div><div class="line">head subject-2017-02-26T00_28_51-2017-02-27T02_15_34.tsv -n 5</div></pre></td></tr></table></figure><pre><code>uid    name    nickname    joindate    activedate7    7    lorien.    2008-07-14    2010-06-052    2    陈永仁    2008-07-14    2017-02-178    8    堂堂    2008-07-14    2008-07-149    9    lxl711    2008-07-14    2008-07-14name    iid    typ    state    adddate    rate    tags2    189708    real    dropped    2016-10-06        2    76371    real    dropped    2015-11-07        2    119224    real    dropped    2015-03-04        2    100734    real    dropped    2014-10-09        subjectid    authenticid    subjectname    subjecttype    rank    date    votenum    favnum    tags1    1    第一次的親密接觸    book    1069    1999-11-01    57    [7, 84, 0, 3, 2]    小説:1;NN:1;1999:1;国:1;台湾:4;网络:2;三次元:5;轻舞飞扬:9;国产:2;爱情:9;经典:5;少女系:1;蔡智恒:8;小说:5;痞子蔡:20;书籍:12    2    坟场    music    272        421    [108, 538, 50, 18, 20]    陈老师:1;银魂:1;冷泉夜月:1;中配:1;银魂中配:1;治愈系:1;银他妈:1;神还原:1;恶搞:1;陈绮贞:94    4    合金弹头7    game    2396    2008-07-17    120    [14, 164, 6, 3, 2]    STG:1;结束:1;暴力:1;动作:1;SNK:10;汉化:1;2008:1;六星:1;合金弹头:26;ACT:10;NDS:38;Metal_Slug_7:6;诚意不足:2;移植:26    6    军团要塞2    game    895    2007-10-10    107    [15, 108, 23, 9, 7]    抓好社会主义精神文明建设:3;团队要塞:3;帽子:5;出门杀:1;半条命2:5;Valve:31;PC:13;军团要塞:7;军团要塞2:24;FPS:26;经典:6;tf:1;枪枪枪:4;2007:2;STEAM:25;TF2:15</code></pre><p>由于爬虫的性质，这些数据有以下缺陷：</p><ol><li>非实时。我所说的“实时”并不是今天是 4 月 16 日而数据只是 2 月的，而是我无法保证数据是在某一个时间点上的快照。对于用户数据，由于爬取一次需要两天的时间，在这两天的时间里，可能用户修改了他们的昵称和用户名而在爬取的数据上未反映出来。更为严重的问题是，对于收藏数据，可能会出现在爬取数据的时候用户进行了收藏的操作，导致爬取的数据出现重复或缺失。而且由于用户数据和收藏数据是分开爬取的，我无法保证通过用户名能把两个 table 一一对应地 join 起来。</li><li>非顺序。可以从预览的数据中看到。</li><li>爬虫本身缺陷。由于我对于 Bangumi 出现 500 错误没有在处理上体现出来，所以会导致某些数据有所缺失。</li></ol><p>在下面的文章里，我们将一边使用 Unix Command 对数据进行类似于 SQL 语句的操作，一边阐述 Bangumi_Spider 产生的 data 的各种特点和后续处理需要注意的问题。</p><h2 id="1-SELECT-…-WHERE-…-ORDER-BY-…"><a href="#1-SELECT-…-WHERE-…-ORDER-BY-…" class="headerlink" title="1. SELECT … WHERE … ORDER BY …"></a>1. SELECT … WHERE … ORDER BY …</h2><h3 id="筛选-2017-冬季番组"><a href="#筛选-2017-冬季番组" class="headerlink" title="筛选 2017 冬季番组"></a>筛选 2017 冬季番组</h3><p>现在我们有了条目数据， 而条目数据是记录了标签信息的，我们可以从标签信息中抽取出 2017 年冬季番组。这个标签是“2017 年 1 月”。我们可以用一个 grep 语句取出这些番组：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep <span class="string">"2017年1月"</span> subject-2017-02-26T00_28_51-2017-02-27T02_15_34.tsv | grep <span class="string">"anime"</span> | wc -l</div></pre></td></tr></table></figure><pre><code>90</code></pre><p>然而这个抽取方式有很大的缺陷。我们没有指定应该在数据的哪一列上查找“anime”或“2017 年 1 月”！如果有一部音乐条目的名字里面就有 anime 这个词，而又被打上了 2017 年 1 月的标签呢？这显然不是我们希望得到的。实际上，需要指定列的话，最好的方式就是使用 <code>awk</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">"\t"</span> <span class="string">'$9 ~ /[;\t]2017年1月:/ &amp;&amp; $4=="anime"'</span> subject-2017-02-26T00_28_51-2017-02-27T02_15_34.tsv &gt; anime_selection.tsv</div><div class="line">wc -l anime_selection.tsv</div><div class="line">head anime_selection.tsv -n 5</div></pre></td></tr></table></figure><pre><code>85 anime_selection.tsv122772    122772    六心公主    anime        2016-12-30    26    [19, 41, 1, 1, 4]    17冬:1;原创:1;PONCOTAN:4;2016年:2;广桥凉:1;TVSP:1;池赖宏:1;原优子:1;mebae:1;TV:4;日本动画:1;片山慎三:1;Studio:1;STUDIOPONCOTAN:4;2016:5;TVA:1;短片:2;上田繁:1;搞笑:4;中川大地:2;岛津裕之:2;种崎敦美:1;2017年1月:1;テレビアニメ:1;オリジナル:1;SP:1;6HP:2;村上隆:10;未确定:1125900    125900    锁链战记～赫克瑟塔斯之光～    anime    3065    2017-01-07    88    [66, 24, 216, 20, 60]    山下大辉:3;17冬:1;原创:1;游戏改:47;CC:1;花泽香菜:7;TV:22;未确定:2;グラフィニカ:2;佐仓绫音:4;2017年1月:61;锁链战记:1;2017:10;锁链战记～Haecceitas的闪光～:15;热血:2;チェインクロ:1;石田彰:22;声优:2;2017年:4;Telecom_Animation_Film:1;十文字:1;柳田淳一:1;战斗:2;内田真礼:2;剧场版:1;奇幻:17;2017·01·07:1;工藤昌史:3;2015年10月:1;TelecomAnimationFilm:9126185    126185    POPIN Q    anime        2016-12-23    10    [134, 11, 3, 3, 0]    荒井修子:1;黒星紅白:4;原创:3;黑星红白:1;2016年:5;_Q:1;日本动画:1;2016年12月:2;未确定:1;小泽亚李:1;2017:2;2016:5;动画电影:1;2017年:5;Q:3;东映动画:1;种崎敦美:1;2017年1月:1;宫原直树:1;POPIN:6;東映アニメーション:12;剧场版:24;东映:4;萌系画风:1;濑户麻沙美:5129805    129805    混沌子    anime    2910    2017-01-11    197    [264, 24, 764, 60, 67]    上坂すみれ:3;ブリドカットセーラ恵美:2;季番:2;黑暗推理向慎入:2;2016年:3;CHAOS:5;游戏改:67;SILVER_LINK.:2;悬疑:5;游戏:9;TV:73;未确定:9;伪:4;GAL改:106;2017:28;科幻:2;志倉千代丸:4;SILVERLINK.:34;SLIVERLINK.:70;2017年:10;志仓千代丸:4;2017年1月:170;5pb.:112;混沌子:3;反乌托邦:16;剧透禁止:27;极权主义世界:8;松冈祯丞:3;神保昌登:6;CHILD:5131901    131901    神怒之日    anime        2017-10-01    0    [79, 1, 0, 3, 1]    GENCO:3;2017年10月:2;TV:4;未确定:2;2017年:2;GAL改:4;游戏改:4;LIGHT:2;2017:3;エロゲ改:3;2017年1月:1</code></pre><p>可以看到，我们提升了一些 precision。<code>awk</code> 的思想就是把一个文本文件按行处理，然后在单引号里面对行进行编程。你可以看到，我们使用内部变量 \$9 指定第 9 列的内容（你可以看到，是从 1 开始标号的），这一列是标签。我们使用正则表达式对 \$9 进行匹配，正则表达式需要用斜杠包围。同时，我们指定第四列的内容是 anime。这样就可以将满足我们需求的条目筛选出来。顺便说一句，\$0 是整行的内容。</p><h3 id="按照在看人数排序"><a href="#按照在看人数排序" class="headerlink" title="按照在看人数排序"></a>按照在看人数排序</h3><p>我们在条目中用一个数组记录了想看、看过、在看、搁置和抛弃的人数。这个数组用方括号所包围。我现在希望对我刚才抽取的冬季番组按照在看人数从大到小排序。这需要我们抽取出第八列，然后从该列中抽取出在看人数。使用以下命令可以达到我的目的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">"\t"</span> <span class="string">'&#123;match($8, /\[([0-9]+), ([0-9]+), ([0-9]+), ([0-9]+), ([0-9]+)\]/, m); printf("%d\t%s\t%s\t%d\t%d\t%d\t%d\t%d\n", $1, $3, $4, m[1], m[2], m[3], m[4], m[5])&#125;'</span> &lt; anime_selection.tsv | sort -t$<span class="string">'\t'</span> -k6,6 -nr | sed 5q</div></pre></td></tr></table></figure><pre><code>179949    小林家的龙女仆    anime    157    84    2065    19    46185792    小魔女学园    anime    245    51    1471    30    29174043    为美好的世界献上祝福！ 第二季    anime    297    63    1431    21    28174143    人渣的本愿    anime    271    51    1381    60    133188091    珈百璃的堕落    anime    126    46    1366    22    73</code></pre><p>按照管道切割，第一个语句还是使用 <code>awk</code>，但是执行的功能不是筛选，而是构造新表。在这个构造新表的过程中，观看人数的抽取使用了 <code>match</code> 函数。被抽取的对象存储在变量 <code>m</code> 里，是 <code>match</code> 的第三个变量。第一个变量是目标匹配字符串，第二个变量是正则表达式。注意需要将匹配的对象用圆括号括起来。</p><p>awk 的函数有很多，具体的可以直接参考 <a href="https://www.gnu.org/software/gawk/manual/gawk.html" target="_blank" rel="external">awk 手册</a>（我估计没人会看的）或是<a href="https://www.math.utah.edu/docs/info/gawk_13.html" target="_blank" rel="external">这里</a>。很多都是 C 语言的内置函数。</p><p>对于排序功能我们需要调用 <code>sort</code> 命令。首先需要指定输入文件的分隔符（注意这里有点 hacky，必须是 <code>-t$&#39;\t&#39;</code>）。由于我们希望对在看人数从大到小排序，我们必须指定按照第 6 列排序，即 <code>-k6,6</code>。同时我们指定 <code>-nr</code> 表明视第六列为数字并倒序。</p><p>结果显示了在二月某个时刻收看番组的前五个。Very reasonable。</p><h3 id="抽取标签列表"><a href="#抽取标签列表" class="headerlink" title="抽取标签列表"></a>抽取标签列表</h3><p>既然我们已经爬取了标签，能不能对爬取的标签列表进行展开？这时候要使用 <code>awk</code> 的 <code>split</code> 函数和 array 类型了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">"\t"</span> <span class="string">'&#123;split($9, tags, ";");for(i in tags)&#123; split(tags[i], itm, ":"); printf("%d\t%s\t%s\t%d\n", $1, $3, itm[1], itm[2]);&#125;;&#125;'</span> &lt; anime_selection.tsv | sort -t$<span class="string">'\t'</span> -k1,1n -k4,4nr | head -n 20</div></pre></td></tr></table></figure><pre><code>122772    六心公主    村上隆    10122772    六心公主    2016    5122772    六心公主    PONCOTAN    4122772    六心公主    STUDIOPONCOTAN    4122772    六心公主    TV    4122772    六心公主    搞笑    4122772    六心公主    2016年    2122772    六心公主    6HP    2122772    六心公主    中川大地    2122772    六心公主    岛津裕之    2122772    六心公主    短片    2122772    六心公主    テレビアニメ    1122772    六心公主    オリジナル    1122772    六心公主    17冬    1122772    六心公主    2017年1月    1122772    六心公主    mebae    1122772    六心公主    SP    1122772    六心公主    Studio    1122772    六心公主    TVA    1122772    六心公主    TVSP    1sort: write failed: standard output: Broken pipesort: write error</code></pre><p>由于标签列表是按照分号分隔的，我们首先使用 <code>split($9, tags, &quot;;&quot;)</code> 把分割后的字符串存储在 array <code>tags</code> 里。接着在 <code>for(i in tags)</code> 里，<code>i</code> 实际上是 index，对于每一个 tag 我们再次使用 <code>split</code>，得到具体的 tag 和 tag 人数。可以看到，可以使用 C 语言的方式写 awk 的行处理逻辑。在写的时候是可以隔行的，虽然我都写在了一行。</p><p>在此之后，我们首先对条目的 id 排序，再在条目中对 tag 的标记人数排序。这里 <code>sort</code> 需要使用两个 <code>-k</code> 选项指定排序顺序。同时我们把 <code>-nr</code> 的条件写在每个排序列的后面，这样可以对列按照不同的排序逻辑排序。</p><h2 id="2-Aggregation"><a href="#2-Aggregation" class="headerlink" title="2. Aggregation"></a>2. Aggregation</h2><h3 id="计数"><a href="#计数" class="headerlink" title="计数"></a>计数</h3><p>我们更为关心的是收藏数据中的一些统计数据，如平均分、活跃人数等。最基本的统计数据形式就是计数。虽然可以使用 awk 完成，但是这个比较特殊，有更为简单的方法。</p><p>比如说，我们想在上文抽取出来的冬季番组标签中统计每个番组会有多少标签。鉴于我们已经把标签对每个动画展开了，我们就可以对每个动画出现的次数进行统计，就可以得到该动画对应的标签数量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cut -f1,2 &lt; anime_taglist.tsv| uniq -c | sed 10q</div></pre></td></tr></table></figure><pre><code>29 122772    六心公主30 125900    锁链战记～赫克瑟塔斯之光～25 126185    POPIN Q30 129805    混沌子11 131901    神怒之日30 143205    南镰仓高校女子自行车社29 146732    碧蓝幻想30 148037    伤物语III 冷血篇30 148099    刀剑神域：序列之争30 148181    飙速宅男 新世代</code></pre><p>其中，<code>cut</code> 命令对我们抽取出来的标签列表的第一列和第二列单独取出，然后送给 <code>uniq</code>。 <code>uniq</code> 会对每行进行 de-duplication 的操作，并且加上 <code>-c</code> 的选项会给出每行出现的个数。在输出的内容中，第一列就是每个动画对应的标签数量。</p><p><strong>需要重点强调的是，uniq 只能对已经排序的输入有效。</strong></p><p>可以看到，在一个条目页面，标签数量最多只有 30 个。</p><h3 id="最大、最小值"><a href="#最大、最小值" class="headerlink" title="最大、最小值"></a>最大、最小值</h3><p>有时候我们会对列表的某几项求取最大、最小值。下面显示了如何求取每一部动画标记人数最多的 tag。当然，我们也是用 <code>awk</code> 完成的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">"\t"</span> <span class="string">'$1!=prev &#123;print $0; prev=$1&#125;'</span> &lt;anime_taglist.tsv | sed 10q</div></pre></td></tr></table></figure><pre><code>122772    六心公主    村上隆    10125900    锁链战记～赫克瑟塔斯之光～    2017年1月    61126185    POPIN Q    剧场版    24129805    混沌子    2017年1月    170131901    神怒之日    GAL改    4143205    南镰仓高校女子自行车社    2017年1月    34146732    碧蓝幻想    A-1Pictures    38148037    伤物语III 冷血篇    剧场版    80148099    刀剑神域：序列之争    剧场版    87148181    飙速宅男 新世代    2017年1月    36</code></pre><p>请注意，受了 <code>uniq</code> 的启发，这里的处理也是需要输入已经排好序。</p><h3 id="平均值"><a href="#平均值" class="headerlink" title="平均值"></a>平均值</h3><p>既然有了 <code>awk</code>，我们就可以做更多的复杂计算。这里我们用平均值举一个例子：我们希望对爬取的收藏记录挑出动画和有评分的，计算其平均值。我还是希望能在有序输入上计算，于是先生成有序输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed 1d record-2017-02-20T14_03_27-2017-02-24T10_57_16.tsv | awk -F <span class="string">"\t"</span> <span class="string">'$6 &amp;&amp; $3=="anime"'</span> | sort -k2,2n | cut -f7 --complement &gt; anime_record.tsv</div><div class="line">head anime_record.tsv</div></pre></td></tr></table></figure><pre><code>103122    2    anime    do    2012-10-18    9103909    2    anime    collect    2012-11-06    8104394    2    anime    collect    2012-12-28    9110320    2    anime    collect    2012-12-11    10112414    2    anime    collect    2012-12-26    7118090    2    anime    collect    2013-01-31    7125406    2    anime    collect    2013-03-04    8165363    2    anime    collect    2013-10-10    6183190    2    anime    collect    2014-02-01    8207963    2    anime    collect    2014-07-21    8</code></pre><p>然后按照下式计算每个动画的平均值，并把平均值从高到低排序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">"\t"</span> <span class="string">'&#123;cnt[$2]+=1; cum[$2]+=$6&#125;; END &#123;for(i in cnt)&#123;printf("%d\t%f\t%d\n", i, cum[i]/cnt[i], cnt[i]);&#125;&#125;'</span> &lt; anime_record.tsv | sort -t$<span class="string">'\t'</span> -k2,2nr -k3,3nr | head</div></pre></td></tr></table></figure><pre><code>202419    10.000000    4186960    10.000000    3143694    10.000000    2158396    10.000000    2193619    10.000000    211121    10.000000    1127124    10.000000    1127125    10.000000    1127597    10.000000    1137405    10.000000    1sort: write failed: standard output: Broken pipesort: write error</code></pre><p>需要注意的是，这里又使用了 <code>awk</code> 里面的字典数据结构（associative array）。你可以看作是一个 python 里面的 <code>dict</code>。我们使用了两个变量：<code>cnt</code> 和 <code>cum</code> 存储每一个动画 id 的评分人数和评分总和。在最后，我们在 <code>END</code> 包围的代码块里面生成最后的结果。这时候 <code>END</code> 里面的语句是在文件遍历之后执行的。</p><h2 id="3-Union-intersection-and-except"><a href="#3-Union-intersection-and-except" class="headerlink" title="3. Union, intersection and except"></a>3. Union, intersection and except</h2><h3 id="在用户记录中丢失的用户"><a href="#在用户记录中丢失的用户" class="headerlink" title="在用户记录中丢失的用户"></a>在用户记录中丢失的用户</h3><p>在前文我们讲过，用户数据可能会因爬虫爬取时出现 500 错误而丢失，但是条目记录可能保留了这部分数据。而且，由于用户数据爬取的时间先于收藏数据，会出现用户在其间改了用户名、还有新的注册用户加入 Bangumi 等问题。这里我们试着查看有多少这类在用户数据中丢失的用户。</p><p>首先我们用用户数据生成用户 id 和 username 的列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed 1d user-2017-02-17T12_26_12-2017-02-19T06_06_44.tsv| cut -f1,2 | sort -t$<span class="string">'\t'</span> -k1,1n &gt; user_list.tsv</div><div class="line">tail user_list.tsv</div></pre></td></tr></table></figure><pre><code>321167    321167321168    321168321169    321169321170    321170321171    321171321172    321172321173    321173321174    321174321175    321175321176    321176</code></pre><p>还有条目数据中的用户列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed 1d record-2017-02-20T14_03_27-2017-02-24T10_57_16.tsv | cut -f1 | sort -t$<span class="string">'\t'</span> -k1,1 | uniq &gt; record_user.tsv</div><div class="line">head record_user.tsv</div></pre></td></tr></table></figure><pre><code>10000410001710001810002610003910004910005110005410006100060</code></pre><p>这实际上就是求 record_user 对于 user_list 中 user 的差集。如果是这样的话，我们可以使用 <code>comm</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cut -f2 user_list.tsv| sort | comm -13 - record_user.tsv &gt; user_failure.tsv</div><div class="line">head user_failure.tsv</div></pre></td></tr></table></figure><pre><code>1237212825914651500017437417601210506223507257848273783</code></pre><p><code>comm</code> 命令对每个文件的行操作，同时它的先验要求是文件已经排过序。它可以给出三列数据：仅在第一个文件中出现的行；仅在第二个文件中出现的行；在两个文件中同时出现的行。可以看出，这个就是求差集和并集的操作。通过指定 <code>-13</code>,我们指定只输出仅在第二个文件中出现的行，也就是在 user_list.tsv 中没有爬到的用户。</p><h2 id="4-Join"><a href="#4-Join" class="headerlink" title="4. Join"></a>4. Join</h2><h3 id="获得收藏数据中的用户-id"><a href="#获得收藏数据中的用户-id" class="headerlink" title="获得收藏数据中的用户 id"></a>获得收藏数据中的用户 id</h3><p>在收藏数据中，我们记录下了用户的用户名，却没有记录用户 id 和用户昵称！这个是爬虫在设计时候的缺陷。这时候只能通过和用户数据 join 来弥补了。可是怎么在文本文件中进行 join 的操作呢？</p><p>首先，我们抽取两组数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed 1d record-2017-02-20T14_03_27-2017-02-24T10_57_16.tsv| sort -t$<span class="string">'\t'</span> -k1,1 &gt; record.sorted.tsv</div><div class="line">head record.sorted.tsv</div></pre></td></tr></table></figure><pre><code>100004    10380    anime    collect    2012-09-30    10    标签:;中二病;花泽香菜;嘟嘟噜;牧瀬紅莉栖;Steins100004    10440    anime    collect    2012-10-02    9    标签:;あの日見た花の名前を僕達はまだ知らない100004    10639    anime    collect    2012-10-02        标签:;虚渊玄;奈绪蘑菇;梶浦100004    16235    anime    collect    2012-10-02    8    标签:;未来日记;我妻由乃100004    18635    anime    collect    2012-10-02    7    标签:;罪恶王冠;泽野弘之100004    23684    anime    collect    2012-09-30    9    标签:;黑子的篮球;基100004    23686    anime    do    2012-10-01        标签:;刀剑神域;2012年7月;SAO;川原砾;梶浦由纪100004    2453    anime    collect    2012-10-02    7    标签:;BRS;Black★RockShooter100004    2463    anime    collect    2012-10-02    8    标签:;デュラララ!!100004    265    anime    collect    2012-09-30    10    标签:;EVA;庵野秀明;神作;Evangelion;补完</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed 1d user-2017-02-17T12_26_12-2017-02-19T06_06_44.tsv| cut -f1,2,3 | sort -t$<span class="string">'\t'</span> -k2,2 &gt; user_list.sorted.tsv</div><div class="line">head user_list.sorted.tsv</div></pre></td></tr></table></figure><pre><code>10    10    ＭｏＭｏ.100    100    Jacob10000    10000    漠漠100000    100000    natalie_1204100001    100001    七堂伽蓝100002    100002    宇100003    100003    二的二次方100004    100004    从不卖萌的K100005    100005    Astrid100006    100006    tsiaben</code></pre><p>需要注意的是，我们希望对用户的用户名进行 join，其前提是我们的列表需要对被 join 的列进行排序。我们已经在上面进行了对用户名列的排序操作。</p><p>接着，我们可以使用 <code>join</code> 了。我们可以首先进行 INNER JOIN：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># inner join</span></div><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 user_list.sorted.tsv record.sorted.tsv | wc -l</div></pre></td></tr></table></figure><pre><code>6483106</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 user_list.sorted.tsv record.sorted.tsv | sed 5q</div></pre></td></tr></table></figure><pre><code>100004    100004    从不卖萌的K    10380    anime    collect    2012-09-30    10    标签:;中二病;花泽香菜;嘟嘟噜;牧瀬紅莉栖;Steins100004    100004    从不卖萌的K    10440    anime    collect    2012-10-02    9    标签:;あの日見た花の名前を僕達はまだ知らない100004    100004    从不卖萌的K    10639    anime    collect    2012-10-02        标签:;虚渊玄;奈绪蘑菇;梶浦100004    100004    从不卖萌的K    16235    anime    collect    2012-10-02    8    标签:;未来日记;我妻由乃100004    100004    从不卖萌的K    18635    anime    collect    2012-10-02    7    标签:;罪恶王冠;泽野弘之join: write error: Broken pipe</code></pre><p>可以看到我们已经 join 了一些有价值的东西。被 join 的那个字段总是在第一列。</p><p>如果使用 LEFT OUTER JOIN 或是 RIGHT OUTER JOIN，我们需要用 <code>-a</code> 指定哪个文件需要全部输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># left outer join</span></div><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 -a 1 user_list.sorted.tsv record.sorted.tsv | wc -l</div></pre></td></tr></table></figure><pre><code>6718272</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 -a 1 user_list.sorted.tsv record.sorted.tsv | head</div></pre></td></tr></table></figure><pre><code>10    10    ＭｏＭｏ.100    100    Jacob10000    10000    漠漠100000    100000    natalie_1204100001    100001    七堂伽蓝100002    100002    宇100003    100003    二的二次方100004    100004    从不卖萌的K    10380    anime    collect    2012-09-30    10    标签:;中二病;花泽香菜;嘟嘟噜;牧瀬紅莉栖;Steins100004    100004    从不卖萌的K    10440    anime    collect    2012-10-02    9    标签:;あの日見た花の名前を僕達はまだ知らない100004    100004    从不卖萌的K    10639    anime    collect    2012-10-02        标签:;虚渊玄;奈绪蘑菇;梶浦join: write error: Broken pipe</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># right outer join</span></div><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 -a 2 user_list.sorted.tsv record.sorted.tsv | wc -l</div></pre></td></tr></table></figure><pre><code>6492074</code></pre><p>这里我们可以看到使用 RIGHT OUTER JOIN 的时候，对于没有被 join 上的用户就是在 user_list 中没有出现的用户。这时候没有被 join 上用户名和用户昵称的记录应该和 user_failure.tsv 的内容是一样的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 -a 2 user_list.sorted.tsv record.sorted.tsv | sort &gt; record.complete.tsv</div><div class="line">join -t$<span class="string">'\t'</span> -1 2 -2 1 user_list.sorted.tsv record.sorted.tsv | sort &gt; record.inner.tsv</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">comm -13 record.inner.tsv record.complete.tsv| cut -f1 | uniq | diff - user_failure.tsv</div></pre></td></tr></table></figure><pre><code>140d139&lt; sdf4141a141&gt; sdf4</code></pre><p>可以看到两个文件在实际内容上，除了顺序有所差别，并没有本质上的变动。这一定程度上证明了 JOIN 的正确性。</p><p>当然，我们既然有了 <code>join</code>，就会有人问怎么进行多个 column 的 join？遗憾的是，目前 <code>join</code> 只支持一列的 join。对于多列的 join 可以参考<a href="http://stackoverflow.com/a/26370525" target="_blank" rel="external">这个</a>。</p><h2 id="5-Conclusion-and-future-plan-for-Bangumi-Spider"><a href="#5-Conclusion-and-future-plan-for-Bangumi-Spider" class="headerlink" title="5. Conclusion and future plan for Bangumi Spider"></a>5. Conclusion and future plan for Bangumi Spider</h2><p>依据现有的技术，我们可以通过配合 <code>awk</code> 与 <code>sort</code> 进行文本文件操作从而组合成各种我们想要的类似于 SQL 语句的功能，这个给我们线下调试带来了很大的方便。同时我们可以使用 <code>cat</code>、<code>comm</code> 达到交集、并集和差集的功能，使用 <code>join</code> 可以把两个文件通过指定的列 join 起来。在懒得使用 pandas 或数据库的时候，这些命令给我们带来了很大的方便。</p><p>同时，我觉得目前 Bangumi Spider 问题太多。我计划从一下方面入手将 Bangumi Spider 升级：</p><ol><li>废除 user spider，更新 record spider 使之记录 user spider 目前所记录的所有信息，在爬虫结束任务之后，通过 raw record spider 生成 record 记录和 user 记录。后期处理还要力求 record spider 不能重复爬取条目。</li><li>修改 Bangumi Spider 对 Bangumi 主站 500 错误的处理。</li><li>保留 subject spider 的部分功能，特别是 infobox 那边的功能，去掉 tag 信息。 subject 要和 record 相 join 解决条目重定位的问题。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在处理业务上的某些事情时，会发现这样的问题:为了调试某个程序，会 dump 出一堆中间文件的 log，去查找哪些地方发生了异常。这种文件都是 tsv 文件，即使用 TAB 分割的列表文件。一种最简单的做法就是在文本编辑器中打开这些文件，然后通过观察去查看异常，可以配合编
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="tools" scheme="https://wattlebird.github.io/tags/tools/"/>
    
      <category term="linux" scheme="https://wattlebird.github.io/tags/linux/"/>
    
      <category term="coreutils" scheme="https://wattlebird.github.io/tags/coreutils/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的疯狂三月——NCAA 男篮预测竞赛</title>
    <link href="https://wattlebird.github.io/2017/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%96%AF%E7%8B%82%E4%B8%89%E6%9C%88%E2%80%94%E2%80%94NCAA-%E7%94%B7%E7%AF%AE%E9%A2%84%E6%B5%8B%E7%AB%9E%E8%B5%9B/"/>
    <id>https://wattlebird.github.io/2017/03/18/机器学习的疯狂三月——NCAA-男篮预测竞赛/</id>
    <published>2017-03-18T14:48:38.000Z</published>
    <updated>2017-03-18T14:49:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章我们讨论 <a href="https://en.wikipedia.org/wiki/NCAA_Division_I_Men%27s_Basketball_Tournament" target="_blank" rel="external">NCAA 一级男子篮球锦标赛</a>的比赛结果预测。这同样是一个 Kaggle 例行举行的<a href="https://www.kaggle.com/c/march-machine-learning-mania-2017" target="_blank" rel="external">算法竞赛</a>。这篇文章所要讲的就是我的解题思路。不过鉴于这是赛前预测，真正的结果需要到四月才能得知，众看官在察觉这是一口毒奶之后大可拿真正比赛结果打我脸。</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>这个算法竞赛给出了如下数据：从 1985 年到 2017 年所有常规赛季的得分记录和 2003 年到 2017 年所有常规赛季的详细记录。详细记录包括一堆复杂的篮球统计数字，由于敝人并不会打篮球所以也不知道中文翻译成什么合适。同时也包括 1985 年到 2016 年所有锦标赛季的得分记录和从 2003 到 2016 的锦标赛季详细。所要预测的是 2017 年进入锦标赛的所有 68 支队伍的所有可能组合的获胜概率——由于每一支队伍都已经标上了序号，预测的是每一对组合中序号较小的队伍的获胜概率。所以需要预测 $\frac{68 \times 67}{2}=2278$种情况。该比赛的目标函数是使得比赛实际出现的组合的获胜情况的 log loss 最小。</p><p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right]<br>$$</p><p>其中 n 是实际会发生的比赛的次数。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>由于我对体育比赛一无所知，经过论坛的讨论观察，我决定<strong>仅</strong>拿当年的常规赛季预测当年的锦标赛季结果。如果你去读了 NCAA 的比赛规则之后，你会发现一个最明显的 feature：seed。Seed 一定程度上包含了比赛队伍的排名强弱信息。<a href="https://www.kaggle.com/ajniggles/march-machine-learning-mania-2017/logistic-regression-and-game-round-calculator" target="_blank" rel="external">在论坛里有人告诉我们</a>，如果仅仅拿 seed 进行一个简单的预测：seed 小的一方必定会赢过 seed 大的一方，就已经能够得到 72.75% 的准确率了。然而 accuracy 与 logloss 还是有很大差别的，这个数字也只是给人们一个选取 feature 的直观感受而已。经过实验，如果单单使用两队 seed 之差一个 feature 进行 logistic regression 的 training，使用 2003-2012 的数据作为 training set，2013-2016 的数据作为 test set，可以得到 0.62 左右的 logloss。（论坛里有人拿到了 0.55 左右的 logloss，这是事后预测结果，不可应用于事前预测。）</p><p>当然这个结果就是给人保个底。之后就可以在这个基础之上进行模型增强了。经过多方挖掘，我又发现了一些奇妙的统计数据：<a href="http://www.basketball-reference.com/about/factors.html" target="_blank" rel="external">Four Factors of Basketball Success</a>. 把这些 feature 加入之后，使用 LR 可以得到 0.60 左右的 logloss. 其实还可以自己造一些类似的 feature，比如说 $\frac{Assist}{FGA+0.44\times FTA+Assist}$。这些也有益于提升模型性能。但是这些提升聊胜于无，让我深感失望。</p><p>我决定使用我写的无监督排序库 <a href="https://github.com/wattlebird/ranking" target="_blank" rel="external">rankit</a> 提升模型性能。在 rankit 里，我提供了 Massey rank, Colley rank, Point difference rank, Markov Rank, Offence-Defence rank 还有 Keener rank. 光是这些 rank 方法在 得分上堆砌起来就能获得很多 feature 了。rating 比 ranking 能提供更多的信息，所以我使用两队的 rating 之差作为 feature. 至于 normalization, 我对每一个 feature 的 Cumulative distribution function 把所有 feature normalize 到 [0, 1]. 到这里，如果使用 MPN，可以获得 0.58 左右的 logloss.</p><p>然而 rankit 仅仅是对比赛得分进行排名，也太浪费了。我们有那么多统计数据，我们可以对那么多的统计数据每个进行一次 rating 的计算，可以得到更有价值的信息。如果我们对两支队伍的防守篮板进行 rating 计算，和对进攻篮板的 rating 计算在物理意义上就有防守能力与进攻能力的差别。于是我们可以把所有统计数据用所有可能的无监督排序方法计算一下 rating.</p><p>于是，最终使用的 model 采用了两类 feature：一种是统计数据的直接简单归纳，如篮板计算赛季均值，助攻计算赛季均值，加上上文提到的 Four factors of basketball success。还有一类 feature 则是对统计数据的无监督排序处理化。</p><p>那么，每一队都有了能够足够表示自己的 feature，是不是应该直接相减得到 两队发生一次比赛的 feature，继而用这个 feature 训练一个 LR 或 GBDT 进行 classification 呢。事实上我用的是 CNN。为什么看上了 CNN 呢，我把表示一场比赛的 feature 组织成一个两行的矩阵，每一行就是描述一个队伍的 feature。如果你使用 CNN，你加一个卷积核在上面，你比使用两个 feature 直接相减有了更多的控制参数：至少对于两支队伍同一个 feature 你的特征抽取方式是 $c_1~x_1+c_2~x_2+c_3$，而如果使用 feature 直接相减就相当于 $c_1=-c_2$，少了模型的控制自由度。而且，我希望得到 feature 之间相互 cross 的效果，使用一种连接主义模型就能得到这样的效果。当然有人就会说了，你这样会不会有过拟合啊，因为你这个模型复杂度太大了。事实上，我的实验表明使用一个很简单的四层神经网络（输入层，卷积层，全连接层，输出层）就能很好地得到我的效果，而且比 GBDT （使用 xgboost 和 lightgbm 做的实验，feature 是两支队伍的 feature 相减）要好。</p><p>这个模型的代码在<a href="https://github.com/wattlebird/KaggleMarchMania2017/blob/master/March%20Machine%20Learning%20Mania%202017.ipynb" target="_blank" rel="external">这里</a>。</p><h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p>我一开始的设想还是 feature cross，也就是构造一些队伍之间相互交互的 feature 达到更加细粒度的队伍实力描述，然后我发现这个甚至不如直接使用 seed delta train 一个 lr. 我认为其主要原因是训练数据太少：由于是对锦标赛进行预测，而可用的训练数据也只有两千多个，我用一个十万维度的 model 能 train 出什么东西来？</p><h2 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h2><p>现在的 rankit 非常难用，连我用的时候都不得不去查看源代码确认用法或是确认是否已经添加了我希望的功能。我希望今年的锦标赛结束之前能把 rankit 升级到 0.2. </p><p>另外，我在上文说过，我仅仅使用当前赛季的常规赛比赛情况预测锦标赛比赛情况。但是一直队伍在历史上的长期表现应该也会对该季锦标赛产生影响，但是我没有做。使用历史数据的麻烦就在于生成特征需要一年一年地生成，sequentially.</p><p>此外，论坛中也有提到使用 elo 评分作为特征。Elo 评分体系是一个重要的体系，却没有被加入 rankit。在比赛最后两天我试图实现这个，但是时间太少，就没有做。这也是一个待办事项。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章我们讨论 &lt;a href=&quot;https://en.wikipedia.org/wiki/NCAA_Division_I_Men%27s_Basketball_Tournament&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NCAA 一级男子篮球
      
    
    </summary>
    
      <category term="Competition" scheme="https://wattlebird.github.io/categories/Competition/"/>
    
    
      <category term="Machine Learning" scheme="https://wattlebird.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Max flow, minimum cut</title>
    <link href="https://wattlebird.github.io/2016/10/11/Max-flow-minimum-cut/"/>
    <id>https://wattlebird.github.io/2016/10/11/Max-flow-minimum-cut/</id>
    <published>2016-10-11T09:53:04.000Z</published>
    <updated>2016-10-11T09:57:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>在微软苏州已经入职了一个月了，生活开始渐渐稳定下来。和在北京的各种不堪相比，我要感谢这次调职，得以让我从 easy 模式开始社畜生活。同时搁置好久的算法又可以得以继续研究下去了。</p><p>今天讲的算法是最大流最小割算法。实际上这个是我在《算法导论》上看来看去都看不明白的一个章节：术语太多，而且代码很少（我好菜啊）。幸运的是，hihoCoder 上面的一系列专题为我垫了不少砖头，让我得以掌握这一块知识。</p><p>对于一个最大流问题，其定义一般都是这样的：有一个有向图 G，源点为 s，汇点为 t，每条边都有其对应的流最大容量 c，源点 s 只有发出的流量，汇点 t 只有汇入的流量，求能从源点到汇点的最大可能流量。</p><p>那么，我先不想说证明过程，我直接给出结论：我们用一种反复查找的方法，试图在当前图 G 上找到一条从 s 到 t 的路径，其边上允许的流量非零。每次找到这一条路径之后，也就可以确定这条路径上的流量瓶颈，将路径上的边的可行流量减去这一流量瓶颈，在新图上进行下一次查找。我们一直持续这样的查找，直到无法从 s 到 t 走出一条流量瓶颈大于 0 的路径。这个算法叫做 Ford-Fulkerson 算法。这个查找方式可以使用 BFS 进行。</p><p>嘛，基本上就是这样。但是由于图中可能会存在反向边，对于我上面讲的这个情况，对于下面这种情况是不成立的：</p><p><img src="flow.jpg" alt="An tricky example of flow network"></p><p>如果我们用 BFS 找到了 A-B-D-F 这条可行路径，其瓶颈是 3，减去该瓶颈之后无法找到更新的路径。然而其最大流是 5：由 B 分出的流量有一份到 D，另外两份到 E，然后 A-C-D 有一个流量为 2 的路径。这时候，我们需要一个“残余网络”的概念帮助我们解决这个问题：每条边维护一条对应的反向边，其大小为当前在正向上面已经消耗掉的流量，而正向边的容量为当前正向剩余的可行流量。换句话说，正向边和反向边的存在，帮助我们维护了每条边还有多少剩余流量可用。如果有一条流量容量为 1 的边，其正向流量为 1000000，反向流量为 999999，这也是可行的！我们在残余网络上找到的一条可行路径，叫做增广路径。于是我上面描述的算法可以如下表达：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">While ( findAugmentPath() ) <span class="comment">// 判断是否有增广路</span></div><div class="line">maxFlow = maxFlow + delta <span class="comment">// 最大流增加</span></div><div class="line">modifyGraph() <span class="comment">// 对增广路进行修改</span></div><div class="line">End While</div></pre></td></tr></table></figure><h2 id="与最小割的关系"><a href="#与最小割的关系" class="headerlink" title="与最小割的关系"></a>与最小割的关系</h2><p>我们通常说 max flow, minimun cut。那么什么被定义为 cut？在一个带有 s 和 t 的网络流里，有一种划分把点划分到两个不相交的 S 和 T 集合中，$s \in S, t \in T$. 净流 f(S, T) 被定义为穿过割 (S, T) 的流量之和（当一个割经过反向边的时候，反向边上的流量应为负值）。割的容量 C(S, T) 被定义为这条割上所有的容量之和（不包括反向边）。也就是说，$f(S, T) \le C(S, T)$。 可以证明，当前网络的流量总是等于任意一个割的净流。但是，任意的割不会有相同的容量。其中最小的那个割的容量对应的割，我们称之为最小割。</p><p>现在有一个结论：对于任意网络流图来说，最大流一定等于最小割的容量。这个结论是最大流最小割定理的直接推论，这个定理由三个等价的表达组成：</p><ol><li>f 是网络的最大流；</li><li>该网络的残余网络不包含任何增广路径。</li><li>流网络的某个切割 (S, T) 的容量等于 f。</li></ol><p>因此，求出了最大流，也就求出了最小割的最大容量。使用上文所提及的 Ford-Fulkerson 算法，能够快速算出割、网络流量。</p><h2 id="Production-Code"><a href="#Production-Code" class="headerlink" title="Production Code"></a>Production Code</h2><p>使用 BFS 进行每次增广路径查找的 Ford-Fulkerson 实现叫做 Edmonds-Karp 算法。由于 BFS 时间复杂度（约）为 O(E)，流量递增操作的总次数为 O(VE)，该算法的时间复杂度为 $O(VE^2)$。这并不是最快的算法。现在有一种算法，通过记录每个点到汇点 t 的最短距离维持搜索顺序，使用 DFS 的方法进行增广路径的查找，能大大降低时间复杂度。这一算法的代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 本代码对应于 http://hihocoder.com/problemset/problem/1369</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">505</span>, maxm = <span class="number">40005</span>;</div><div class="line"><span class="keyword">int</span> N, M;</div><div class="line"></div><div class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></div><div class="line"><span class="keyword">int</span> u, v, c, prev;</div><div class="line">&#125;<span class="keyword">edge_t</span>;</div><div class="line"><span class="keyword">int</span> head[maxn];</div><div class="line"><span class="keyword">edge_t</span> edges[maxm];</div><div class="line"></div><div class="line"><span class="keyword">int</span> source, sink;</div><div class="line"><span class="keyword">int</span> dep[maxn], depcnt[maxn], cur[maxn], <span class="built_in">stack</span>[maxn];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">addedge</span><span class="params">(<span class="keyword">int</span> u, <span class="keyword">int</span> v, <span class="keyword">int</span> c)</span> </span>&#123;</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> id = <span class="number">0</span>;</div><div class="line">edges[id].u = u; edges[id].v = v; edges[id].c = c; edges[id].prev = head[u]; head[u] = id++;</div><div class="line">edges[id].u = v; edges[id].v = u; edges[id].c = <span class="number">0</span>; edges[id].prev = head[v]; head[v] = id++;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">bfs</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="built_in">memset</span>(dep, <span class="number">0xff</span>, <span class="keyword">sizeof</span>(dep));</div><div class="line"><span class="built_in">memset</span>(depcnt, <span class="number">0</span>, <span class="keyword">sizeof</span>(depcnt));</div><div class="line">dep[sink] = <span class="number">0</span>;</div><div class="line">depcnt[dep[sink]] = <span class="number">1</span>;</div><div class="line"><span class="built_in">std</span>::<span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; Q;</div><div class="line">Q.push(sink);</div><div class="line"><span class="keyword">int</span> u, v;</div><div class="line"><span class="keyword">while</span> (!Q.empty()) &#123;</div><div class="line">u = Q.front();</div><div class="line">Q.pop();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> e = head[u]; ~e; e = edges[e].prev) <span class="keyword">if</span> (edges[e].c == <span class="number">0</span> &amp;&amp; dep[edges[e].v] == <span class="number">-1</span>) &#123;</div><div class="line">v = edges[e].v;</div><div class="line">Q.push(v);</div><div class="line">dep[v] = dep[u] + <span class="number">1</span>;</div><div class="line">depcnt[dep[v]]++;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SAP</span><span class="params">()</span> </span>&#123;</div><div class="line">bfs();</div><div class="line"><span class="built_in">memcpy</span>(cur, head, <span class="keyword">sizeof</span>(head));</div><div class="line"><span class="keyword">int</span> u = source, maxflow = <span class="number">0</span>, top = <span class="number">0</span>, i;</div><div class="line"><span class="keyword">while</span> (dep[source]&lt;N) &#123;</div><div class="line"><span class="keyword">if</span> (u == sink) &#123;</div><div class="line"><span class="comment">//find the bottleneck</span></div><div class="line"><span class="keyword">int</span> delta = inf, p;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> t = top - <span class="number">1</span>; t &gt;= <span class="number">0</span>; t--) &#123;</div><div class="line"><span class="keyword">int</span> e = <span class="built_in">stack</span>[t];</div><div class="line"><span class="keyword">if</span> (edges[e].c&lt;delta) &#123;</div><div class="line">delta = edges[e].c;</div><div class="line">p = t;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> t = top - <span class="number">1</span>; t &gt;= <span class="number">0</span>; t--) &#123;</div><div class="line"><span class="keyword">int</span> e = <span class="built_in">stack</span>[t];</div><div class="line">edges[e].c -= delta;</div><div class="line">edges[e ^ <span class="number">1</span>].c += delta;</div><div class="line">&#125;</div><div class="line">maxflow += delta;</div><div class="line">top = p;</div><div class="line">u = edges[<span class="built_in">stack</span>[p]].u;</div><div class="line">&#125;</div><div class="line"><span class="keyword">for</span> (i = cur[u]; ~i; i = edges[i].prev) &#123;</div><div class="line"><span class="keyword">if</span> (edges[i].c&gt;<span class="number">0</span> &amp;&amp; dep[edges[i].v] == dep[u] - <span class="number">1</span>) &#123;</div><div class="line">cur[u] = i;</div><div class="line">u = edges[i].v;</div><div class="line"><span class="built_in">stack</span>[top++] = i;</div><div class="line"><span class="keyword">break</span>;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (i == <span class="number">-1</span>) &#123;</div><div class="line">depcnt[dep[u]]--;</div><div class="line"><span class="keyword">if</span> (depcnt[dep[u]] == <span class="number">0</span>) <span class="keyword">break</span>;</div><div class="line"><span class="keyword">int</span> mindep = N;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> e = head[u]; ~e; e = edges[e].prev) &#123;</div><div class="line"><span class="keyword">if</span> (edges[e].c&gt;<span class="number">0</span> &amp;&amp; mindep &gt; dep[edges[e].v]) &#123;</div><div class="line">mindep = dep[edges[e].v];</div><div class="line">cur[u] = e;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">dep[u] = mindep + <span class="number">1</span>;</div><div class="line">depcnt[dep[u]]++;</div><div class="line"><span class="keyword">if</span> (u != source) &#123;</div><div class="line">u = edges[<span class="built_in">stack</span>[--top]].u;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> maxflow;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">freopen(<span class="string">"testcase"</span>, <span class="string">"r"</span>, <span class="built_in">stdin</span>);</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;N, &amp;M);</div><div class="line">source = <span class="number">1</span>, sink = N;</div><div class="line"><span class="built_in">memset</span>(head, <span class="number">0xff</span>, <span class="keyword">sizeof</span>(head));</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= M; i++) &#123;</div><div class="line"><span class="keyword">int</span> u, v, c;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d%d%d"</span>, &amp;u, &amp;v, &amp;c);</div><div class="line">addedge(u, v, c);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">printf</span>(<span class="string">"%d\n"</span>, SAP());</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>嘛，这个优化后的算法叫什么我也不知道，我只是从别人的解法里学习过来的。但是这个代码做了至少这么几个工作：</p><ol><li>在进行 DFS 寻找增广路径之前用 BFS 从汇点搜索建立 dep 和 depcnt，即到汇点的最短距离，这样为在 DFS 的时候总是走最短路径提供了依据。</li><li>正向边和反向边成对存储，索引为偶数的总是正向边；正向边和反向边可以通过索引的异或切换；</li><li>stack 里面存储通过 DFS 寻找的增广路径。</li><li>在每一次 DFS 寻找过后，回溯到瓶颈路径的起点重新搜索，节省时间；</li><li>维护一个 cur，其在物理意义上与 head 相同，也是为了直接从当前的可行边搜索，节省时间；</li><li>但是如果从某点找不到可行边，需要更新该点的 dep 与 depcnt。这也就意味着 cur 也要更新。</li></ol><p>就是这样！但是有的文献指出，第一步建立 dep 与 depcnt 可以不进行。这主要的实现参考 <a href="http://hihocoder.com/contest/hiho117/solution/898175。" target="_blank" rel="external">http://hihocoder.com/contest/hiho117/solution/898175。</a></p><p>主要参考文献：</p><ol><li>hihocoder hiho一下 115、116、117、118、119 周</li><li>《算法导论》（第三版）第 26 章</li><li><a href="http://www.cnblogs.com/longdouhzt/archive/2012/05/13/2497770.html" target="_blank" rel="external">Max Flow-SAP-Improved Shortest Augmenting</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在微软苏州已经入职了一个月了，生活开始渐渐稳定下来。和在北京的各种不堪相比，我要感谢这次调职，得以让我从 easy 模式开始社畜生活。同时搁置好久的算法又可以得以继续研究下去了。&lt;/p&gt;
&lt;p&gt;今天讲的算法是最大流最小割算法。实际上这个是我在《算法导论》上看来看去都看不明白
      
    
    </summary>
    
      <category term="OJ review" scheme="https://wattlebird.github.io/categories/OJ-review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
      <category term="Max flow" scheme="https://wattlebird.github.io/tags/Max-flow/"/>
    
      <category term="Minimun cut" scheme="https://wattlebird.github.io/tags/Minimun-cut/"/>
    
  </entry>
  
  <entry>
    <title>滴滴算法大赛总结</title>
    <link href="https://wattlebird.github.io/2016/06/18/%E6%BB%B4%E6%BB%B4%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    <id>https://wattlebird.github.io/2016/06/18/滴滴算法大赛总结/</id>
    <published>2016-06-18T02:31:06.000Z</published>
    <updated>2017-10-02T01:45:57.145Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://research.xiaojukeji.com/competition/main.action?competitionId=DiTech2016&amp;&amp;locale=zh_CN" target="_blank" rel="external">滴滴算法大赛</a>是一场旨在预测城市交通供给需求不平衡程度的机器学习比赛。这个比赛的赛题是给定待预测的时间节点，预测整个城市在不同区块中需求与供给的差异 gap. 最终的比赛衡量方式是</p><span>$$\begin{equation}MAPE = \frac{1}{D}\sum_{d=1}^{D}\left( \frac{1}{T} \sum_{t=1}^{T} \left| \frac{gap_{dt}-s_{dt}}{gap_{dt}}\right|\right) \forall gap_{dt}&gt;0\end{equation}$$</span><!-- Has MathJax --><p>在比赛中，D = 66，T = 144. 官方给出的训练数据有前三个星期的全部订单，天气和路况描述，区块属性描述。</p><p>我对该大赛的代码在<a href="https://github.com/wattlebird/ditech" target="_blank" rel="external">这里</a>。下面我来描述一下我的解决方案。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>在这场比赛中我只使用订单的训练数据，无视天气、路况和区块属性数据。理由是天气、路况和区块属性最终还是会反映在订单上面。所以着重描绘订单反映的特征就可以了。我提取了如下八类特征，全部以 one-hot coding 方式呈现：</p><ol><li>订单所处时间是哪一小时（24维）</li><li>订单时间是否为双休日（2维）</li><li>订单地点在哪一区块（66维）</li><li>7 天前同一时间槽供求状况（21维）</li><li>订单所处时间的前一时间槽供求状况（21维）</li><li>订单前两个时间槽组合供求缺口状况（49维）</li><li>订单前三个时间槽的平均缺口状况（7维）</li><li>订单前三个时间槽的缺口标准差状况（3维）</li></ol><p>“时间槽”是赛题预先定义的把一整天时间划分为 144 个时间片的每一个时间片。为了用 one-hot coding 的方式描述供求，我使用如下的方式重新定义 gap：将 gap 以 [0,0],(0,1],(1,3],(3,6],(6,12],(12,30],(30,$+\infty$) 划分，形成 7 维特征。同时将需求（也就是客户发单数）以 [0,10],(10,20],(20,$+\infty$) 划分，形成 3 维特征。对于 21 维的“供求状况”，则使用 2d feature 的方式将两个特征相组合。对于“缺口状况”，则是将两个 gap 以 2d feature 的方式相组合，形成 49 维特征。对于缺口的标准差，则使用 [0,1],(1,5],(5,$+\infty$) 划分，形成 3 维特征。</p><p>但是这是远远不够的。比如说，我们想要设计的特征，能够包含一个区块在不同时间槽上的变化，这时候就必须结合时间特征和区域特征做二维特征。于是可以对所有这八类特征相互组合形成二维特征，最终形成一个一万多维的、非常稀疏特征。如果这时候按照这个特征训练一个线性回归模型，可以在测试集上面达到 0.32-0.34 的结果。但是实际上在应用的时候，并非八种特征完全相互组合，而是选取有物理意义的组合，以减少参数个数。代码中通过使用矩阵作为 mask 指定哪几类特征组合是被允许的。</p><p>我们还有一维特征，使用量的大小而非 one-hot coding：训练一个线性回归模型，预测的结果形成最后一维特征。这个特征也可以与先前的八类特征相互组合形成二维特征。</p><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><p>一开始我使用 scikit-learn 的线性回归模型建模的，但是这个库可能偏离了实用性。对于稀疏的 feature，我非常希望能有一范数做正则项，但是 scikit-learn 里面没有一个模型是符合我的要求的。而且，对于题目中要求的 metric，最好是对每一个训练样本进行 $\frac{1}{gap_{dt}}$的加权。并不是所有的模型在 scikit-learn 里面都有加权的选项。</p><p>其实最好的工具是 xgboost。这是一个基于残差迭代方法的、专门应用于机器学习竞赛的库。通过使用基于 GBDT 的回归模型，可以达到 0.29 左右的结果。需要指出的是，我发现三次残差迭代的效果是最好的，几乎颠覆了我以前对 GBDT 的印象。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>我选取了元月 4 日、9 日、17 日、19 日和 21 日的全天数据作为 validation，其余的去除元月 1-3 日的数据作为 training. 同时复制 10 日和 16 日的记录一遍。</p><p>需要指出的是，在训练的时候要去除 gap=0 的数据，因为这些数据对评价标准没有任何影响（如果真的像评价标准那样定义的话）而且还会对模型产生 bias。</p><h2 id="可能存在的问题"><a href="#可能存在的问题" class="headerlink" title="可能存在的问题"></a>可能存在的问题</h2><p><a href="https://twitter.com/haruki_kirigaya/status/737212998872244224" target="_blank" rel="external">先前有报导显示</a>，直接下载提交的测试文件就能达到 0.6 的 level，而按照定义，这样的测试文件应该生成 1 的 MAPE。可能我在我的 MAPE 计算程序里面有与官方理解不一致的地方。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我原来以为这个题目还是比较简单的，因为就是一个纯纯的监督学习问题。但是我发现我与最高水平的差距还是挺大。不论是实力还是客观条件方面都与别人的队伍有着较大的差距。最为愤怒的是，在前一天我在知乎上面看到有人说使用前三个时间槽的 gap 进行加权平均就能达到 0.29 的结果，我非常愤怒。我为了达到这个值花了这么多时间和精力，竟然还不如别人这么简单的无脑方法？？？如果能有更多的人和我团队合作，我们的思路就会越来越开阔，不至于像我走这种死胡同。</p><p>而且，我并非从一开始就参加这个竞赛，而是在硕士学位答辩完成之后才想着去“玩一玩”的。先前看到过 <a href="https://twitter.com/haruki_kirigaya" target="_blank" rel="external">@haruki_kirigaya</a> 也在玩这个（不过我要说，我没有看任何人的代码）。第一次提交是在 6 月 9 号，只有十次提交机会，而且中间还换了一次数据。不过，借口是无穷多的，实力的差距是绝对的。这也是我参加的第一次机器学习竞赛。我上次看到一个 Bangumi 上的人 fo 了我（我这种弱渣垃圾有什么好 fo 的），人家声称坚持打过每一场 Kaggle 竞赛，我真是无地自容。还是看书去吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://research.xiaojukeji.com/competition/main.action?competitionId=DiTech2016&amp;amp;&amp;amp;locale=zh_CN&quot; target=&quot;_blank&quot; rel=&quot;exte
      
    
    </summary>
    
      <category term="Competition" scheme="https://wattlebird.github.io/categories/Competition/"/>
    
    
      <category term="Machine Learning" scheme="https://wattlebird.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>使用 rankit 构建更科学的排名</title>
    <link href="https://wattlebird.github.io/2016/02/05/%E4%BD%BF%E7%94%A8-rankit-%E6%9E%84%E5%BB%BA%E6%9B%B4%E7%A7%91%E5%AD%A6%E7%9A%84%E6%8E%92%E5%90%8D/"/>
    <id>https://wattlebird.github.io/2016/02/05/使用-rankit-构建更科学的排名/</id>
    <published>2016-02-05T13:21:33.000Z</published>
    <updated>2016-02-05T14:12:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wattlebird/ranking" target="_blank" rel="external">rankit</a> 是一个使用线性代数和最优化理论为基础的常见排名算法库。这个库是我写的。我说“常见”其实对于绝大部分人来说根本就闻所未闻，但是对于专门研究排名的研究者来说，这里面包含的算法都是比较基础的。自从阅读了一本介绍排名的书籍<a href="https://book.douban.com/subject/25915707/" target="_blank" rel="external">《谁排第一？关于评价和排序的科学》</a>之后，我觉得很有必要把这里面的宝藏介绍给对此一无所知的研究者。作为一个做与机器学习相关研究的研究生，竟然除了 PageRank 和 HITS 其他排名算法一个都没有听说过——课上不教，研究也涉及不到——因为生活中应用这些排名算法的场景，实在是很少接触到。然而，这些排名算法正在美国的体育比赛排名中大行其道。在那本书中，大部分案例直接来源于美国的橄榄球比赛排名。</p><p>然而要我把这本书里面所有的算法都介绍出来，实在是一件难事，一是有侵犯版权的嫌疑，二是其中涉及到大量的数学推导。我在此也只能简单地说一下。每一个排名（rank）由一个既定的评分（rating）产生，评分可以被解释为被排名对象的实力。在一定数学模型的指导下，被排名对象产生了比赛结果。这时候，可以看作 rating 这一因素导致了外在的比赛结果的观测值，这和 state 与 observation 的关系是一样的。根据不同的数学模型，就有了不同的 rating 算法。另外还有一个排名算法不依靠于 rating 数值，它试图给出一个 ranking，使得该排名与比赛的结果最为相符，说白了就是 linear ordering problem.</p><p>那么这种排名算法对我们有什么启示呢？现实生活中好像没有多少排名问题能够用被排名对象之间的比赛来表达啊。搜索引擎的网页排名能用 in-link 和 out-link 表达网页之间的关系，但是网页之间怎么进行比赛呢？事实上，这本书能够拓宽人的眼界的地方就在于此。如果在一个网页排名结果中，网页点击量的大小可以视作在这一场比赛中的结果。网页流量的大小也是网页实力的一个外在特征。在商品排名中，同时购买两件商品的人的评分综合也可以被视为两件商品比赛的结果。事实上，我们应该抛弃“比赛”这种概念，对两个排名对象在同等条件下比较的结果进行 fitting 要远远稳定于计算一个平均分，或是加权平均的结果。</p><h2 id="小白怎样使用-rankit？"><a href="#小白怎样使用-rankit？" class="headerlink" title="小白怎样使用 rankit？"></a>小白怎样使用 rankit？</h2><p>rankit 提供了 Converter，只要用户能提供排名对象的每一轮的比赛结果（用 pandas.DataFrame 表示并包含特定的 columns），就能够为后续的算法计算出矩阵。在不同的数学模型下，需要的观测值就会有所不同。每一个算法都有它的物理意义，那么使用某一个算法要使用与其物理意义相对应的矩阵。比如说对于 MarkovRank 来说，这种算法需要表示排名对象之间相互投票的结果。我在 rankit 里面专门提供了 RateDifferenceVoteMatrix，SimpleDifferenceVoteMatrix 和 RateVoteMatrix 把观测到的评分结果表示成这样的矩阵。对于算法适用于什么样的矩阵这个问题，我在 rankit 的 GitHub 主页上面已经给出了参考表格。</p><p>rankit 还提供了排名融合的算法，排名融合能够使多种算法的排名更加稳定。用户也可以自行构建排名加入排名融合器，生成融合后的排名。另外，如果要比较排名之间的结果，rankit 还提供了两种测度描述排名列表之间的差距。</p><p>尽管我接口做得比较人性化了，我还是希望有一定背景知识的人去操纵这些算法。</p><h2 id="为-Bangumi-动画排名！"><a href="#为-Bangumi-动画排名！" class="headerlink" title="为 Bangumi 动画排名！"></a>为 Bangumi 动画排名！</h2><p>长久以来，Bangumi 用户对 Bangumi 的动画排名争论不休<a href="http://chii.in/blog/269600" target="_blank" rel="external">[1]</a><a href="http://chii.in/blog/269560" target="_blank" rel="external">[2]</a>。由于 Bangumi 网站的高质量，某些别有用心人士试图通过刷分提高某些动画的排名或降低某些动画的排名，达到自己不可告人的宗教和政治目的<a href="http://chii.in/group/topic/337530" target="_blank" rel="external">[3]</a><a href="http://chii.in/group/topic/311903" target="_blank" rel="external">[4]</a><a href="http://chii.in/blog/270420" target="_blank" rel="external">[5]</a>。除此以外，有用户建议使用更为细致化的排名系统<a href="http://chii.in/group/topic/13906" target="_blank" rel="external">[6]</a><a href="http://chii.in/blog/48952" target="_blank" rel="external">[7]</a>，但是所有这些讨论都没有超出统计学的范畴，最多也只是在平均分上做些加加减减，或是玩一些加权的把戏。一个综合的讨论收录可以参考<a href="http://chii.in/group/topic/41475" target="_blank" rel="external">[8]</a>.</p><p>我们提出了一种全新的基于不同数学模型的排名方法，这些排名方法通过比较每两部作品的表现评分。同时看过两部作品的用户，我们可以计算他们为这两部作品评分的算术平均分、对数几何平均分和倾向性概率，从而生成三种作品间两两比较的实力数据。接着，对于每一种生成方法，我们使用八种不同的算法对作品进行排名。这些排名算法由 rankit 提供技术支撑。对于生成的 24 种排名，我们使用 Borda Count 融合所有排名，生成最终的动画排名。这 8 种算法分别是：</p><ol><li>Colley Rank (colley)</li><li>Massey Rank (massey)</li><li>Difference Rank (differ)</li><li>Markov Rank, using rate vote matrix as input (markov_rv)</li><li>Markov Rank, using rate difference vote matrix as input (markov_rdv)</li><li>Markov Rank, using simple difference vote matrix as input (markov_sdv)</li><li>Offence-defence Rank (od)</li><li>Keener Rank (no bias) (keener)</li></ol><p>使用基于算术平均的排名使用 ariavg 做前缀，使用基于对数几何平均的排名使用 geoavg 做前缀，使用概率的排名我们使用 prob 做前缀。我们把最后的排名记作 merged_rank，把 Bangumi 原始排名记作 bangumi_rank，并对每一对排名计算 Kendall Measure，我们得到以下矩阵：</p><p><img src="index.png" alt="Kendall Measure Matrix"></p><p>如果两个排名的 Kendall Measure 等于 1，说明两个排名的相似度比较大。如果越趋近于 -1，则两个排名完全相反。从这张图我们可以看到，使用 Borda Count 后的综合排名与各大排名的 Kenall Measure 都比 Bangumi 原始平均分排名表现要好，这充分说明了我们排名的科学性。</p><p>下表是该综合排名拍出来的 Bangumi 已排名 3252 部动画的前 20 位对应的作品 id：</p><table><thead><tr><th>id</th><th>rank</th></tr></thead><tbody><tr><td>253</td><td>1</td></tr><tr><td>326</td><td>2</td></tr><tr><td>324</td><td>3</td></tr><tr><td>265</td><td>4</td></tr><tr><td>237</td><td>5</td></tr><tr><td>321</td><td>6</td></tr><tr><td>6049</td><td>7</td></tr><tr><td>1728</td><td>8</td></tr><tr><td>110467</td><td>9</td></tr><tr><td>2907</td><td>10</td></tr><tr><td>340</td><td>11</td></tr><tr><td>839</td><td>12</td></tr><tr><td>1608</td><td>13</td></tr><tr><td>876</td><td>14</td></tr><tr><td>3302</td><td>15</td></tr><tr><td>238</td><td>16</td></tr><tr><td>2734</td><td>17</td></tr><tr><td>1428</td><td>18</td></tr><tr><td>120700</td><td>19</td></tr><tr><td>37460</td><td>20</td></tr></tbody></table><h2 id="如何在新算法下操纵排名？"><a href="#如何在新算法下操纵排名？" class="headerlink" title="如何在新算法下操纵排名？"></a>如何在新算法下操纵排名？</h2><p>每一种算法都有被攻克的那一天，在提出语义搜索引擎之前，Google 一直在与 SEO 做着猫鼠游戏。当然，我这种简单的排名算法就更容易被操纵了。</p><p>如果有用户进行刷分，而且每个小号只为一部作品刷分，那么我的排名算法能够阻止这种作弊行为。但是如果有用户进行刷分的小号对多部作品进行刷分，而且是为某几部作品评高分，而给某几部作品评低分，这样就会对排名算法产生影响。</p><p>当然，也存在着针对这种行为的对抗方式，我们只需要对 Keener Rank 进行一些修改就能使得排名不容易被这种作弊方法操纵。</p><p>而更为切实际的做法是，将人工排名与计算机排名相融合。在我看来，最为稳定的融合方式是 LeastViolatedMerge，但是这是一个 NPC 问题，目前还没有对 3000 部动画这种规模的 LeastViolatedMerge 解决方案。所以说越科学的排名需要消耗的资源也就越大。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/wattlebird/ranking&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;rankit&lt;/a&gt; 是一个使用线性代数和最优化理论为基础的常见排名算法库。这个库是我写的。我说“常见”其实对于绝大
      
    
    </summary>
    
      <category term="Projects" scheme="https://wattlebird.github.io/categories/Projects/"/>
    
    
      <category term="Machine Learning" scheme="https://wattlebird.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>欧拉回路</title>
    <link href="https://wattlebird.github.io/2015/06/28/%E6%AC%A7%E6%8B%89%E8%B7%AF/"/>
    <id>https://wattlebird.github.io/2015/06/28/欧拉路/</id>
    <published>2015-06-28T09:16:31.000Z</published>
    <updated>2015-06-28T20:44:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天讨论的主题是一类问题，就是欧拉路问题。有两种欧拉路。第一种叫做 Eulerian path(trail)，沿着这条路径走能够走遍图中每一条边；第二种叫做 Eularian cycle，沿着这条路径走，不仅能走遍图中每一条边，而且起点和终点都是同一个顶点。注意：欧拉路要求每条边只能走一次，但是对顶点经过的次数没有限制。</p><p>满足什么性质的图才能有欧拉路？根据 <a href="https://en.wikipedia.org/wiki/Eulerian_path" target="_blank" rel="external">wikipedia</a> 对欧拉路的介绍：</p><ul><li>在无向图中，所有顶点的度数均为偶，则存在 Eularian cycle；若有且仅有两个顶点的度数为奇，其余的都为偶，则存在 Eularian path；</li><li>在有向图中，所有顶点的入度数等于出度数，则存在 Eularian cycle；若有且仅有两个顶点：其中一个入度数比出度数大 1，另一个入度数比出度数小 1，其余的顶点入度数等于出度数，则存在 Eularian path.</li></ul><p>另外我们还需要知道，对于那些 Eularian path，起点和终点分别在那两个度数为奇的顶点上（对于无向图）或是入度数不等于出度数的顶点上（对于有向图）。</p><p>然而知道这些并没有给我们带来多少实惠。因为我们除了判定一个图有没有欧拉路之外，更想找到其中的一条欧拉路径。于是这就是我们今天的重点：寻找欧拉路径的算法。</p><p>一个比较经典的算法是 Fleury 算法。Fleury 算法的思想就是：在过河拆桥之前，先想想有没有退路。为什么这么说？Fleury 算法每个回合进行到一个顶点上的时候，都会删除已经走过的边。在选择下一条边的时候，不应该出现这样的状况：在删除下一条边之后，连通图被分割成两个不连通的图。除非没有别的边可选择。该算法从一个奇度数顶点开始（若所有顶点度数均为奇，则任选一个顶点）。当所有的边都走完的时候，该算法结束，欧拉路径为删除路径的顺序。用算法伪代码描述就是：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">v_0 &lt;- <span class="keyword">a</span> vertex <span class="keyword">with</span> odd degree <span class="keyword">or</span>, <span class="keyword">if</span> no such vertex, <span class="keyword">any</span> arbitrary vertex.</div><div class="line">Repeat:</div><div class="line">    select <span class="keyword">an</span> vertex v_i+<span class="number">1</span> adjacent <span class="keyword">of</span> v_i, which should <span class="keyword">not</span> separate <span class="keyword">the</span> graph <span class="keyword">or</span>, <span class="keyword">the</span> only adjacent vertex <span class="keyword">of</span> v_i</div><div class="line">    remove edge &lt;v_i, v_i+<span class="number">1</span>&gt; <span class="keyword">and</span> jump <span class="built_in">to</span> v_i+<span class="number">1</span></div><div class="line">Until all edges have been visited.</div><div class="line">Return <span class="keyword">the</span> sequence <span class="keyword">of</span> visited edges.</div></pre></td></tr></table></figure><p>但是该算法的问题就是，怎么判断一条边是否是一个桥呢？如果使用 Tarjan 算法判断，则算法运行时间就是 $O(E^2)$。在实际写代码的时候，我可没考虑那么多。我只考虑，如果在某一点处深搜的结果导致图被分离，那么在某一个边必然走过了一个桥，那么就返回走另一条边。这样的思想形成的算法如下：</p><script src="//gist.github.com/12a01616817d8ab33260.js"></script><p>粗略分析一下，由于算法要经过每条边，所以时间必然是$\Omega(E)$。在最坏情况下，在每个节点处进行一次 DFS，节点会重复走所以以边计算，所以算法复杂度应该是 $O(E(E+V))$。</p><p>另一种计算欧拉路的算法是 Hierholzer 算法。这种算法是基于这样的观察：</p><p><img src="14341858378253.png" alt="Hierholzer"></p><p>在手动寻找欧拉路的时候，我们从点 4 开始，一笔划到达了点 5，形成路径 4-5-2-3-6-5。此时我们把这条路径去掉，则剩下三条边，2-4-1-2 可以一笔画出。</p><p>这两条路径在点 2 有交接处（其实点 4 也是一样的）。那么我们可以在一笔画出红色轨迹到达点 2 的时候，一笔画出黄色轨迹，再回到点 2，把剩下的红色轨迹画完。</p><p>由于明显的出栈入栈过程，这个算法可以用 DFS 来描述。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">DFS</span>(u):</div><div class="line"><span class="selector-tag">While</span> (u存在未被删除的边e(u,v))</div><div class="line">删除边<span class="selector-tag">e</span>(u,v)</div><div class="line"><span class="selector-tag">DFS</span>(v)</div><div class="line"><span class="selector-tag">End</span></div><div class="line"><span class="selector-tag">PathSize</span> ← <span class="selector-tag">PathSize</span> + <span class="selector-tag">1</span></div><div class="line"><span class="selector-tag">Path</span><span class="selector-attr">[ PathSize ]</span> ← <span class="selector-tag">u</span></div></pre></td></tr></table></figure><p>如果想看得更仔细一点，下面是从点 4 开始到点 5 结束的 DFS 过程，其中 + 代表入栈，- 代表出栈。</p><p>4+ 5+ 2+ 3+ 6+ 5+ 5- 6- 3- 1+ 4+ 2+ 2- 4- 1- 2- 5- 4-</p><p>我们把所有出栈的记录连接起来，得到</p><p>5-6-3-2-4-1-2-5-4</p><p>诸位看官可以自己再选一条路径尝试一下。不过需要注意的是，起始点的选择和 Fleury 要求的一样。</p><p>这个算法明显要比 Fleury 高效，它不用判断每条边是否是一个桥。我写的代码如下：</p><script src="//gist.github.com/befdfc0a4802cac3bdfc.js"></script><p>需要注意的是这个算法时间复杂度是 $O(E)$。其在 DFS 的过程中不用恢复边，靠出栈记录轨迹。</p><p>Fin</p><p>（本文大量论述受到 hihocoder week 49, 50 和 51 启发。代码所解决的题目为 <a href="http://hihocoder.com/contest/hiho50/problem/1" target="_blank" rel="external">http://hihocoder.com/contest/hiho50/problem/1</a>）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天讨论的主题是一类问题，就是欧拉路问题。有两种欧拉路。第一种叫做 Eulerian path(trail)，沿着这条路径走能够走遍图中每一条边；第二种叫做 Eularian cycle，沿着这条路径走，不仅能走遍图中每一条边，而且起点和终点都是同一个顶点。注意：欧拉路要求
      
    
    </summary>
    
      <category term="OJ review" scheme="https://wattlebird.github.io/categories/OJ-review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>分解质因数</title>
    <link href="https://wattlebird.github.io/2015/05/25/%E5%88%86%E8%A7%A3%E8%B4%A8%E5%9B%A0%E6%95%B0/"/>
    <id>https://wattlebird.github.io/2015/05/25/分解质因数/</id>
    <published>2015-05-25T08:38:22.000Z</published>
    <updated>2015-05-25T15:39:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天要说的题目来源于 <a href="http://codeforces.com/contest/546/problem/D" target="_blank" rel="external">Soldier and Number Game</a>. 这道题到底干什么呢，说白了就是求两个数$1 \le b \le a \le 5 000 000$之间的所有数的质因数数目之和。快速求解一个数的质因数数目是这道题的关键。</p><p><a href="http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" target="_blank" rel="external">Sieve of Eratosthenes</a> 是这道题的关键。这是一个求解质数/质数判定的方法，其思想是筛选出一个质数的整数倍，质数的整数倍必然不是质数，所以都被筛选出去。在没有筛选的结果中继续求解当前最小数字的整数倍，直至筛完为止。其 C++ 代码如下所示：</p><figure class="highlight cpp"><figcaption><span>Sieve of Eratosthenes</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// prime number table from 2 to 5000000</span></div><div class="line"><span class="comment">// those who have number 0 are prime numbers.</span></div><div class="line"><span class="keyword">int</span> table[<span class="number">5000001</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_table</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> limit = <span class="built_in">sqrt</span>(<span class="number">5000000</span>)+<span class="number">1</span>;</div><div class="line">    table[<span class="number">0</span>]=table[<span class="number">1</span>]=<span class="number">-1</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=limit; i++)</div><div class="line">        <span class="keyword">if</span>(!table[i])</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">2</span>; j*i&lt;=<span class="number">5000000</span>; j++)</div><div class="line">                table[i*j]=i;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">isprime</span><span class="params">(<span class="keyword">int</span> i)</span></span>&#123;<span class="keyword">return</span> !table[i];&#125;</div></pre></td></tr></table></figure><p>你可以发现，在这个质数表中实际上还存储了每个数字的一个质因数。要提取出一个数的完整质因数分解，只要依据<s>基本法</s><code>a/=table[a]</code>不断迭代就可以了。</p><p>但是如果按照这样的思路去数每个数的质因数个数的话，在原题中还是会超时。其一个原因是我们要求的是质因数的个数，其可以在建立质数表的时候完成。其二，求从 a 到 b 中每个质因数的个数之和，倾向于使用部分和的方法。</p><p>那么怎么求每个数的质因数个数呢，在 Sieve of Eratosthenes 的基础之上？</p><figure class="highlight cpp"><figcaption><span>Number of Prime Factors</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_table</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="comment">//const int limit = sqrt(5000000)+1;</span></div><div class="line">    <span class="comment">//table[0]=table[1]=-1;</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=<span class="number">5000000</span>; i++)</div><div class="line">        <span class="keyword">if</span>(!table[i])</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=i; j&lt;=<span class="number">5000000</span>; j+=i)</div><div class="line">                table[j]=table[j/i]+<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天要说的题目来源于 &lt;a href=&quot;http://codeforces.com/contest/546/problem/D&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Soldier and Number Game&lt;/a&gt;. 这道题到底干什么呢，说白
      
    
    </summary>
    
      <category term="OJ review" scheme="https://wattlebird.github.io/categories/OJ-review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Combination Sum</title>
    <link href="https://wattlebird.github.io/2015/04/26/Combination-Sum/"/>
    <id>https://wattlebird.github.io/2015/04/26/Combination-Sum/</id>
    <published>2015-04-26T04:08:44.000Z</published>
    <updated>2015-05-01T19:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Combination sum <a href="https://leetcode.com/problems/combination-sum/" target="_blank" rel="external">1</a> and <a href="https://leetcode.com/problems/combination-sum-ii/" target="_blank" rel="external">2</a> 是 Leetcode 上面两道比较相似的题目。今天写这个题目是因为这个题目有不同的解法。</p><p>这两道题目都是给定一组数，给定一个 target，然后求出这组数中哪几个可以组合成 target。不同的地方是，第一道题要求数字可以被重复利用以组合成 target；第二道题要求数字不可以被重复利用。Seems easy, hmm?</p><p>首先说说我的想法和做法。首先通过观察可以得知，求一个 target 的 combination 可以被拆分成给定某个已知数的剩下数值的 combination 的子问题。于是乎，可以在确定一个 candidate 的同时试图解决这个子问题，子问题应该返回一个所有解的列表，把该 candidate 加入所有解的尾部即可。</p><p>那么怎么选取 candidate 呢？我们可以先对原候选数组进行排序，从最接近 target 的最大数开始筛选 candidate。</p><p>但是这里面还有不少问题。在解决子问题的时候，父问题应该传递给子问题这样的信息，使得子问题在选择 candidate 的时候，不应该出现重复的解。比如说在数组<code>[2,3,6,7]</code>，target 为 7 的时候，当目前确定一个 candidate 为 2，子问题就是相同数组中 target 为 5 的问题。但是此时是否能选择 candidate 为 3 呢？这就与 target 为 7 时 candidate 选为 3 的时候相重复了啊。为了力避这种情况，我规定父问题必须传给子问题当前的 candidate，使得子问题在选择 candidate 之时永远小于父问题 candidate。</p><p>所以解决问题的代码如下：</p><figure class="highlight cpp"><figcaption><span>Combination Sum I</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; combinationSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;candidates, <span class="keyword">int</span> target) &#123;</div><div class="line">        sort(candidates.begin(),candidates.end());</div><div class="line">        <span class="keyword">return</span> dfs(candidates,target,INT_MAX);</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; dfs(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;candidates, <span class="keyword">int</span> target, <span class="keyword">int</span> pred)&#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_iterator itr;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; ans;</div><div class="line">        itr = upper_bound(candidates.begin(), candidates.end(), target);</div><div class="line">        <span class="keyword">if</span>(itr==candidates.begin()) <span class="keyword">return</span> ans;</div><div class="line"></div><div class="line">        <span class="keyword">do</span>&#123;</div><div class="line">            itr--;</div><div class="line">            <span class="keyword">int</span> t = *itr;</div><div class="line">            <span class="keyword">if</span>(t&gt;=pred) <span class="keyword">continue</span>;</div><div class="line">            <span class="keyword">if</span>(!(target%t)) ans.push_back(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(target/t,t));</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=target/t; i++)&#123;</div><div class="line">                <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; vst = dfs(candidates, target-i*t, t);</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">size_t</span> j=<span class="number">0</span>; j!=vst.size(); j++)</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>; k!=i; k++) vst[j].push_back(t);</div><div class="line">                copy(vst.begin(), vst.end(), back_inserter(ans));</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">while</span>(itr!=candidates.begin());</div><div class="line">        <span class="keyword">return</span> ans;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><p>下面第二道题，第一道题之后应该不是什么难题，因为所有的数只能选择一次，所以在选择 candidate 的时候用不着去尝试多次选取的情况了。但是这里还有一个问题：位置不同、但是“看上去”结果相同的解被算作同一个！</p><p>为了优雅地解决这个问题，我规定在选取下一个 candidate 的时候，需要跳过与目前 candidate 相同的值。（注意第 22 行）</p><p>所以这个问题解决的代码如下：</p><figure class="highlight cpp"><figcaption><span>Combination Sum II</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; combinationSum2(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;candidates, <span class="keyword">int</span> target) &#123;</div><div class="line">        sort(candidates.begin(), candidates.end());</div><div class="line">        <span class="keyword">return</span> dfs(candidates.rbegin(),candidates.rend(),target);</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; dfs(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_reverse_iterator b, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_reverse_iterator e,</div><div class="line">            <span class="keyword">int</span> target)&#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_reverse_iterator itr;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; ans;</div><div class="line">        itr = upper_bound(b,e,target,greater_equal&lt;<span class="keyword">int</span>&gt;());</div><div class="line">        <span class="keyword">if</span>(itr==e) <span class="keyword">return</span> ans;</div><div class="line"></div><div class="line">        <span class="keyword">while</span>(itr!=e)&#123;</div><div class="line">            <span class="keyword">int</span> t = *itr;</div><div class="line">            <span class="keyword">if</span>(t==target) ans.push_back(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>,t));</div><div class="line">            <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; vs = dfs(++itr, e, target-t);</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i!=vs.size(); i++)&#123;</div><div class="line">                vs[i].push_back(t);</div><div class="line">                ans.push_back(vs[i]);</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">while</span>(itr!=e &amp;&amp; *itr==t) itr++;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> ans;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><p>但是，以上这些方法都是沿用了 dfs 的思想。既然问题能被拆分为子问题，那么为何不用 DP？这当然是一个正当的思路。</p><p>我们记 target 从 0 到 target 的所有解的数组为 dp[target+1]，最终的结果就是要求 dp[target]。显然我们有 dp[target]=vector<int>({itm.push_back(candidate) for itm in dp[target-i]})（抱歉这种不伦不类的语法）</int></p><p>但是状态转移方程还没有明确从哪里取得 candidate。其实只要从所有 candidate 里面一个一个抽取出来就可以了。</p><p>所以说解决第一题的伪代码如下：</p><figure class="highlight swift"><figcaption><span>Combination Sum I in DP</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">dp[<span class="number">0</span>]=vector&lt;vector&lt;int&gt;&gt;()</div><div class="line"><span class="keyword">for</span> <span class="built_in">c</span> <span class="keyword">in</span> candidates:</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="built_in">c</span>, target]:</div><div class="line">        如dp[j-<span class="built_in">c</span>]非空：</div><div class="line">        dp[j] = 把dp[j-<span class="built_in">c</span>]中的每一个解都push_back一个<span class="built_in">c</span></div></pre></td></tr></table></figure><p>对于第二题用 DP 的话必须配合 Hash set。</p><p>另外，您发现了，用 dp 解的话与背包问题（0-1背包和可重复背包）有什么相似之处吗？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Combination sum &lt;a href=&quot;https://leetcode.com/problems/combination-sum/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;1&lt;/a&gt; and &lt;a href=&quot;https://leetco
      
    
    </summary>
    
      <category term="OJ Review" scheme="https://wattlebird.github.io/categories/OJ-Review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>How I build up Chi: 同步率改 v0.3 实现细节</title>
    <link href="https://wattlebird.github.io/2015/03/08/How-I-build-up-Chi-%E5%90%8C%E6%AD%A5%E7%8E%87%E6%94%B9-v0-3-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/"/>
    <id>https://wattlebird.github.io/2015/03/08/How-I-build-up-Chi-同步率改-v0-3-实现细节/</id>
    <published>2015-03-07T17:57:46.000Z</published>
    <updated>2017-10-02T01:46:28.215Z</updated>
    
    <content type="html"><![CDATA[<p>　　我想我已经好久都没写日志了，因为每天都会学到太多的新东西，虽然想记录下来，但实在是有心无力——太耗时间。不过我想仍然有里程碑意义的成果必须记录下来。这个<a href="http://ikely.me/chi/similarity" target="_blank" rel="external">同步率改 v0.3</a> 就是一个。</p><p>　　Bangumi 番组计划有一个可以内置的查看用户间同步率的功能，但是这个功能很简陋，也很不科学。我通过爬取 Bangumi 用户作品收藏信息，计算出了比较科学一点的同步率，使得品味相近的用户可以互相认识。</p><p>　　稍有常识的人就会看出，这实际上就是一个推荐系统嘛：推荐与你相似的用户。但是目前的同步率改不能完全算是一个推荐系统，只能说起到了某些推荐系统的功能而已。现在的“同步率改”之所以还保留着“同步率”的称法，是为了让 BGMer 在旧同步率和新同步率之间保持无缝概念接受：如果我没有标注某部作品，那么这部作品不应该在同步率上有所贡献。所以在到目前为止，所有的同步率计算都是在作品空间中进行的而非概念空间。</p><p>　　同步率改的 v0.2.1 版是一个比较基本的推荐系统实现：提取用户的所有评分并减去其平均值，得到用户对作品的偏好程度。同时对于用户没有评分的作品，利用这个作品所处于的五个状态——想看、在看、看过、搁置和抛弃——加上一定评分偏置。同时为了防止只有很少评分的用户影响系统表现，系统做了不少 trick，如评分在标准差在零点几以下的用户同步率无视评分、收藏小于 3 的用户无视评分等。这样的设计方法虽然整体思路是正确的，但是存在 ad hoc 太多、评分偏置单一的现象。</p><p>　　同时，在 Bangumi 社区交流的过程中，我发现用户对于不同类型的作品评分有一定的独立性。对于动画和三次元的评分准则，有些用户将其与音乐、书籍类作品相独立看待。</p><p>　　基于这样的事实，我在 v0.2.1 的基础上进行了大量修改，其结果就是 v0.3。他们一脉相承的特点都是：相似度在作品空间中计算，每位用户的向量都以其自己平均分为基准，对已收藏但未评分作品根据作品收藏状态有一定偏置。但是 v0.3 使用了子空间分解技术估计用户已收藏和未评分作品的评分。同时，不同类型作品的同步率分开计算，最后整合在一起形成综合同步率。</p><h2 id="评分估计模型"><a href="#评分估计模型" class="headerlink" title="评分估计模型"></a>评分估计模型</h2><p>　　我们记 Bangumi 某一类型（比如动画）作品的 user-item utility matrix 为 $R$，其是一个 $M \times N$ 的矩阵，其中用户数为 $M$，作品数为 $N$。矩阵的元素是元素所在行对应的用户评分减去该用户评分的平均值。用户未评分的元素值记为 0。我们的目的，就是要对某一个用户已标记未评分、收藏状态为 $s$ 的作品进行评分预估：  </p><span>$$\begin{equation} r_{ij}=u_i v_j^T+bu_{is}+bi_{js}\end{equation}$$</span><!-- Has MathJax --><p>　　其中 $u_{i}$ 是 $M \times Q$ 的 user-concept matirx $U$ 的第 $i$ 行行向量，$v_j$ 是 $Q \times N$ 的 concept-item matrix $V$ 的第 $j$ 列列向量。$bu_{is}$ 是用户状态偏置矩阵 $Bu$ 的元素，$bi_{js}$ 是作品状态偏置矩阵 $Bi$ 的元素。得出评分估计模型的关键就是求出这四个矩阵 $U$、$V$、$Bu$ 和 $Bi$，使得 root mean square error (RMSE)最小，同时又有较好的泛化能力。</p><span>$$\begin{equation}RMSE = \frac{1}{\mbox{total rated item number}}\sum\left(r_{ij}-u_i v_j^T-bu_{is}-bi_{js}\right)^2\end{equation}$$</span><!-- Has MathJax --><p>　　实际上，这个问题就是最小化 RMSE 的最优化问题。假定我们已经有了 $U$、$V$、$Bu$ 和 $Bi$ 的初始估计值，那么我们就可以通过梯度下降的方法估计出使得 RMSE 最小的局部最优解。</p><p>　　在 RMSE 的表达式中，total rated item number 是一个固定的常数，$r_{ij}$ 是已经观测到的评分。我们把最优化的目标函数写成如下形式：</p><span>$$\begin{equation}Obj = \sum\left(r_{ij}-u_i v_j^T-bu_{is}-bi_{js}\right)^2+\frac{1}{2\sigma^2}\left(\sum_i||u_i||^2+\sum_j||v_j||^2+\sum_i||bu_i||^2+\sum_j||bi_j||^2\right)\end{equation}$$</span><!-- Has MathJax --><p>　　目标函数的第一项是 RMSE，第二项是为了防止 $U$、$V$、$Bu$ 和 $Bi$ 过大的罚项。此目标函数对于行向量 $u_i$ 的导数为：</p><span>$$\begin{equation}\frac{\partial}{\partial u_i}Obj = -2\sum_j\left(r_{ij}-u_i v_j^T-bu_{is}-bi_{js}\right)v_j+\frac{1}{\sigma^2}u_i\end{equation}$$</span><!-- Has MathJax --><p>　　对于其他项的导数可以一一推得，而且计算也很简单，这里就不赘述。</p><p>　　但是目前的问题就是如何求出初始的 $U$、$V$、$Bu$ 和 $Bi$？初始值非常重要，选取一个好的初始值可以得到全局最优解。对于 $U$ 和 $V$，其是 $R$ 的一个近似分解。我们可以用 SVD 求出 $U$ 和 $V$ 的初始值。对于 $Bu$，其是一个 $M \times 5 $的矩阵，其每一行的元素是该用户处于想看、在看、看过、搁置或抛弃作品评分平均值与所有作品评分平均值之间的偏差。$Bi$ 是一个 $N \times 5$ 的矩阵，其每一行元素是该作品在 $R$ 中的不同状态中的评分平均值与 $R$ 中的总平均值之间的偏差。</p><h2 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h2><p>　　我们已经推导出了变量的初始值和梯度。通过不断迭代更新 $U$、$V$、$Bu$ 和 $Bi$ 我们可以得到很低的 RMSE。但是矩阵计算量巨大，我们需要在评分系统预估的表现和现有计算能力中取得折中。到底迭代多少次、学习率为多少时合适？</p><p><img src="Anime-iterate.png" alt="Anime iterate"></p><p>　　上图是在动画类别中的 RMSE 随迭代次数变化的变化场景。我按照 7:2:1 的比例将所有动画作品评分划分为训练集、验证集和测试集（后来我发现其实这个阶段还不要用测试集）。可以看出，RMSE 在训练集上可以训练到很低，但是更多的迭代次数对验证集的 RMSE 下降作用不大。因此，我选取了在验证集 RMSE 下降曲线的第二个拐点，即迭代次数约为 70 次的位置作为迭代次数。</p><p>　　另外一个参数是学习率的选取。根据经验，学习率选取在 0.0002 时为合适。但即使如此，在计算动画类的时候，学习率在 0.0002、罚项 $\sigma^2$ 在 10 的时候还会发散。因此动画类的学习率在迭代 10 次之后，以 0.00002 的学习率继续迭代下降。</p><h2 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h2><p>　　在预估出所有已收藏未评分的作品评分之后，我仍然采用余弦距离计算用户之间的同步率。正如上文所说，这样用户未收藏的作品不会影响同步率。目前阶段，我不想让用户看到某位和他/她同步率很高的人看的是完全不同的动画——尽管它们有相同的概念。</p><p>　　在同步率过去的反馈里，我总是听到有人抱怨有收藏了个位数作品的人进入了他们的前十榜单。但是，只要同步率还是采取这样的保守定义：即在作品空间的余弦距离，这种现象永远不会得到根治。</p><p>　　设想一下，如果用户 A 看过的作品是你的子集，而用户 B 看过更多的作品，他不仅看过 A 的作品，而且还看过更多的你没看过的其他作品，那么可能谁会更高？这个答案毫无疑问是 A 与你的同步率更高——尽管你可能更想让 B 在你的同步率榜单中靠前一些。也就是说，相同条件下，余弦距离的定义使得系统必然偏向收藏数目少的人。</p><p>　　鉴于这样的事实，我想同步率这个玩具，v0.3 就是它最后的一个版本了。如果要以“寻求同好”为目的，就必须抛弃“同步率”这样陈旧的概念。</p><h2 id="下一步？"><a href="#下一步？" class="headerlink" title="下一步？"></a>下一步？</h2><p>　　同步率改的设计，我想已经到达了一个里程碑了，而且我想这也完成了我对 matrix factorization 的实践。但是，寻找同好这一终极目标并没有到达。在接下来，对同好的描述可能将不是余弦距离这样直观的模型就能理解的。我已经想好了会怎么做。所以请期待下一个更棒的玩具吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　我想我已经好久都没写日志了，因为每天都会学到太多的新东西，虽然想记录下来，但实在是有心无力——太耗时间。不过我想仍然有里程碑意义的成果必须记录下来。这个&lt;a href=&quot;http://ikely.me/chi/similarity&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
      <category term="Projects" scheme="https://wattlebird.github.io/categories/Projects/"/>
    
    
      <category term="Machine Learning" scheme="https://wattlebird.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Misha and Permutations Summation</title>
    <link href="https://wattlebird.github.io/2015/01/17/Misha-and-Permutations-Summation/"/>
    <id>https://wattlebird.github.io/2015/01/17/Misha-and-Permutations-Summation/</id>
    <published>2015-01-16T17:01:14.000Z</published>
    <updated>2017-10-02T01:46:50.058Z</updated>
    
    <content type="html"><![CDATA[<p>　　此题为 CF 上的一道中等难度题。题目的原文见<a href="http://codeforces.com/contest/501/problem/D" target="_blank" rel="external">这里</a>。用中文简单说一下吧：给定0到N-1的两个排列，求这两个排列的“和”。之所以会有“和”的概念出现，是因为排列的大小可以依据字典序确定。</p><p>　　所以，解决这个问题需要的两个关键步骤是：根据排列确定字典序大小；根据字典序大小还原排列。</p><p>　　一个排列和其字典序大小是有某种确定关系的。<a href="https://en.wikipedia.org/wiki/Factorial_number_system" target="_blank" rel="external">Factorial number system</a> 就这种关系给出了理论支持。这种关系，是一种可以一一对应并且互相转换的关系。比如说，当 N=3 的时候，所有的排列和其字典序大小关系如下所示：</p><table><thead><tr><th>排列</th><th>字典序大小</th></tr></thead><tbody><tr><td>012</td><td>0</td></tr><tr><td>021</td><td>1</td></tr><tr><td>102</td><td>2</td></tr><tr><td>120</td><td>3</td></tr><tr><td>201</td><td>4</td></tr><tr><td>210</td><td>5</td></tr></tbody></table><p>　　要完成互相转换的任务，必须借助于 factorial number system。这种 system 和通常的十进制、十六进制很相似，只是把“基”换成了数乘。比如说，如何表示 42，利用 factorial number system？ 4! &lt; 42 &lt; 5!</p><span>$$\begin{aligned}42 &amp;amp; =  1 \times 4! + 3 \times 3! + 0 \times 2! + 0 \times 1! + 0 \times 0! \\　 &amp;amp; =  (((1 \times 4 + 3) \times 3 + 0) \times 2 + 0) \times 1 + 0 \\　 &amp;amp; =  13000_{!}\end{aligned}$$</span><!-- Has MathJax --><p>　　可以看到，首先 42 小于 5!，因此 42 可以由 5 以下的阶乘表示。<strong>在拆分的时候，每一位的数字不得大于当前位的阶乘数字</strong>。如$13000_{!}$的最高位为 1，1&lt;4。各位看官可以自行拆解几个数字看看。</p><p>　　由此，我们可以把第一个表再增加一项，表示出每一个字典序大小的阶乘数系表示：</p><table><thead><tr><th>排列</th><th>阶乘数系</th><th>字典序大小</th></tr></thead><tbody><tr><td>012</td><td>000</td><td>0</td></tr><tr><td>021</td><td>010</td><td>1</td></tr><tr><td>102</td><td>100</td><td>2</td></tr><tr><td>120</td><td>110</td><td>3</td></tr><tr><td>201</td><td>200</td><td>4</td></tr><tr><td>210</td><td>210</td><td>5</td></tr></tbody></table><p>　　那么在阶乘系统下表示出来的数字，好像和排列没有明显的联系啊。好像还不如用原始的方法，就是直接从排列计算出字典序大小呢。那么这一过程怎么计算呢，用最原始的思路？比如说我们现在有排列 120。其第一个数是 1，那么在第一个数是 0 的时候就已经有了 2! 次排列。再看第二位，是 2。在 2 的前面已经有了 0 的排列（这时候就要去掉 1 了），所以其在第一位数为 1 的情况下第二位数为 2 的之前的排列有 1! 个。再看第三位，是0，这时候排序就已经确定了，加上先前的所有排列，就是 2!+1!+1=4。鉴于字典序从 0 开始计数，减去 1 即可。</p><p>　　可以看到，在使用原始方法计算字典序的时候，在计算后面的数字的时候要记住前面已经使用了什么数字（看官可以拿一个更大的排列计算一下）。因此，我们如果按照这种思路计算的话，很快就会陷入频繁的大小比较中。</p><p>　　如果我们维护一个从 1 到 N 大小的数组，每个数组元素都是 1，并且再维护一个数组，计算其部分和，就会发现，每一次确定某一位之前出现过的数字等于部分和。如果一个数字被用掉了，将其减一，相应的每一位部分和表示该位之前没有被用掉的数字个数。在阶乘系统下表示的数字的每一位，其物理意义就是部分和。我们就此举一个例子：</p><p>　　数组为(1,1,1)，部分和为(1,2,3)，排列为120。数组从 1 开始计数，为了方便，排列每一位加 1 变成 231.</p><ul><li>排列第一位数字 2 索引位置对应的部分和为 2，将其记录下来，并把该位减一. 这时候数组为(1,0,1), 部分和为(1,1,2)；</li><li>排列第二位数字 3 索引位置对应的部分和为 2，将其记录下来，并把该位减一. 这时候数组为(1,0,0), 部分和为(1,1,1)；</li><li>排列第三位数字 1 索引位置对应的部分和为 1，将其记录下来，并把该位减一。</li></ul><p>　　我们记录下来的部分和表示了在这样一个排序中，当某一位没有采用该数字的时候可选择的数字个数。由于排列已经确定，我们把记录下来的部分和减一，变成 110。这样，通过部分和，我们就可以很快计算出排列对应的阶乘系统中的数字。看官可以自己拿一个数字计算一下。</p><p>　　现在我们解决了<strong>从排列到字典序大小的转换问题</strong>，下面要解决的问题就是<strong>字典序大小到排列的转换</strong>了。如何用传统方法解决？我们可以看到阶乘系统下的数字每一位都有“部分和”的意思。同时排列的每一位数字都是不同的。利用这些信息可以从最高位推出排列。首先阶乘系统下的数字的最高位前面没有被选择的数字，它既是部分和，也是排列中的确定数字。在从排列转化为字典序的过程中，确定的数字被减一，变成不可利用，因此部分和中可以利用的部分和只有第一次出现的某个部分和————因为不可利用的数字其值为 0，在部分和中表示为没有变化。</p><p>　　假设我们现在有字典序 2，其对应的阶乘系统数字为 100，我们将其转化为 211。同样地，我们维护数组(1,1,1)和其部分和(1,2,3)。</p><ul><li>阶乘系统下数字最高位为 2，部分和中第一个出现 2 的索引位置为 2，记录该位置，并将该位置减一，现在部分和为(1,1,2);</li><li>阶乘系统下数字次高位为 1，部分和中第一个出现 1 的索引位置为 1，记录该位置，并将该位置减一，现在部分和为(0,0,1)</li><li>阶乘系统下数字最低位为 1，部分和中第一个出现 1 的索引位置为 3，记录该位置。</li></ul><p>　　看官可以发现，之所以记录下来的索引位置是不重复的，是因为每次选取第一个部分和导致了避开使用过的索引位置。记录下来的索引位置为 213，每一位减一等于 102. 看官也可以自己拿一个数字计算一下。</p><p>　　那么在实际操作中，怎么计算部分和并更新部分和呢？如果一个位置发生了改变，以后所有的部分和都要发生改变。一个直接的方法就是通过维护线段树，但是更方便的方法是使用<a href="http://baike.baidu.com/view/1420784.htm" target="_blank" rel="external">树状数组</a>。</p><p>　　树状数组并不直接管理部分和，而是管理某一段的和。对一个长度为 N+1 的数组，其相对应的树状数组与之等长，并且有以下性质：树状数组索引为 n 处的元素管理着索引区间为 [n-lowbit(n)+1, n] 处原始数组区间之和。所谓 lowbit，是指某一个数转化为二进制后最低非零位置转化为一个 2 的整数次方的值。其为 x &amp; (-x)。总而言之，这个数组有着与 2 的整数次方相关联的树状组织。</p><p>　　<img src="tree_array.jpg" alt="Tree Like Array"><br>　　那么要计算部分和，就可以根据这个区间的信息，一块区间一块区间地叠加。可以预见，这样计算部分和的时间复杂度为 O(log n)。</p><p>　　那么如何维护树状数组？我们大可以构建一个原始数组然后在其上构建树状数组，但是鉴于这个问题的部分和是按照索引递增的，而且只在原始数组上执行加减一的操作，我们可以直接构建基于树状数组的加减一操作：在包含当前索引处的区间值加减一即可。可以预见，这样维护树状数组的时间复杂度为 O(log n)。</p><p>　　到了这里，我想解这道题所需要的所有知识已经明确了。下面是我本人写的代码：</p><script src="//gist.github.com/3e6f9a51806662d7fdbf.js"></script><p>　　总结：</p><ol><li>排列与指数系统有着一一对应的关系。排列每个数物理意义为部分和索引，指数系统每一位物理意义为部分和。</li><li>树状数组每一个元素试图管理一个 2 的整数次方的大小的长度的数组，lowbit 运算可以实现这一点。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　此题为 CF 上的一道中等难度题。题目的原文见&lt;a href=&quot;http://codeforces.com/contest/501/problem/D&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这里&lt;/a&gt;。用中文简单说一下吧：给定0到N-1的两个
      
    
    </summary>
    
      <category term="OJ Review" scheme="https://wattlebird.github.io/categories/OJ-Review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>OJ 总结(Depricated)</title>
    <link href="https://wattlebird.github.io/2015/01/03/OJ-%E6%80%BB%E7%BB%93-Depricated/"/>
    <id>https://wattlebird.github.io/2015/01/03/OJ-总结-Depricated/</id>
    <published>2015-01-02T16:00:00.000Z</published>
    <updated>2015-05-01T21:31:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="2015-01-04"><a href="#2015-01-04" class="headerlink" title="2015.01.04"></a>2015.01.04</h3><p><a href="https://oj.leetcode.com/problems/search-insert-position/" target="_blank" rel="external">Simple Binary Search</a></p><p>题目大意就是一种更好的 Binary Search，在一个已经排好序的数组里面找到 target，返回 target 索引。如果找不到，那么就返回 target 应该插入的位置。</p><p>我觉得还是 <em>Acclerated C++</em> 那本书给了我很多启示。Binary  search 的搜索区间看作是一个左闭右开区间，每次迭代的过程中，不变的准则是：区间的右边是开区间。这样可以省去考虑中间点恰好是 target 的麻烦。这样做的好处除了省去中间点的考量外，也省略掉了尾端的考虑：在 C 语言中，整数类型向 0 取整，所以 4 是 end 的话，所有小于 4 的数与之相加除以 2 的结果都必然小于 4。</p><p>当然这样没考虑区间左端。有一种情况区间左端小于 target，这时候就简单地和等于的情况合并喽。</p><p>这道题没用迭代。用迭代还可以再缩短时间。</p><p><a href="https://oj.leetcode.com/problems/merge-sorted-array/" target="_blank" rel="external">Merge Sorted Array</a></p><p>给定两个已排好序的数组，把它们 merge 成一个。不要再分配空间。</p><p>按照惯常的思路是从每个数组的开头开始一一比较然后插入，但这不是 Merge Sort。其实可以从每个数组尾端开始，从大到小插入。但是这样会不会污染原有数据呢？用数学方法证明一下即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;2015-01-04&quot;&gt;&lt;a href=&quot;#2015-01-04&quot; class=&quot;headerlink&quot; title=&quot;2015.01.04&quot;&gt;&lt;/a&gt;2015.01.04&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://oj.leetcode.com/probl
      
    
    </summary>
    
      <category term="OJ Review" scheme="https://wattlebird.github.io/categories/OJ-Review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Note - week 3</title>
    <link href="https://wattlebird.github.io/2014/11/01/Game-Theory-Note-week-3/"/>
    <id>https://wattlebird.github.io/2014/11/01/Game-Theory-Note-week-3/</id>
    <published>2014-10-31T16:00:00.000Z</published>
    <updated>2015-05-01T20:21:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>This week’s material is dedicated to concepts “beyond Nash Equilibrium”: iterative removal of strictly dominated strategies, minimax strategies and the minimax theorem for zero-sum game, correlated equilibria.</p><p>Different from previous weeks’ notes, I will illustrate some concepts in Chinese.</p><h2 id="Dominated-Strategies"><a href="#Dominated-Strategies" class="headerlink" title="Dominated Strategies"></a>Dominated Strategies</h2><p>Strictly dominated strategies are based on every player’s rationality: that is, everybody is rational, and everybody knows other players make rational decisions, and everybody knows that also… So, those strategies that are strictly dominated are to be dominated. We can get final strategy by using iterated removal.</p><h3 id="Example-1-Prisoner’s-dilemma"><a href="#Example-1-Prisoner’s-dilemma" class="headerlink" title="Example 1: Prisoner’s dilemma"></a>Example 1: Prisoner’s dilemma</h3><table><thead><tr><th>Prisoner 1 and 2</th><th>Co</th><th>Be</th></tr></thead><tbody><tr><td>Co</td><td>-0.5, -0.5</td><td>-10, 0</td></tr><tr><td>Be</td><td>0, -10</td><td>-2, -2</td></tr></tbody></table><p>Follow the rational thinking, both prisoners will chose betrayal because choosing betrayal will definitely have him/her spent somewhat less time in prison in comparison to choosing cooperation. In this case, we say that betrayal dominates cooperation.</p><p>What’s interesting is that the final strategy, which is a Nash Equilibrium, is not optimal in a overall view. This is where dilemma lies, which illustrates that 非零和博弈中，帕累托最优和纳什均衡是相冲突的.</p><h3 id="Example-2-Intelligent-Pigs"><a href="#Example-2-Intelligent-Pigs" class="headerlink" title="Example 2: Intelligent Pigs"></a>Example 2: Intelligent Pigs</h3><p>The illustration of this game can be found <a href="http://wiki.mbalib.com/wiki/%E4%B8%A5%E6%A0%BC%E5%8A%A3%E5%8A%BF%E7%AD%96%E7%95%A5" target="_blank" rel="external">here</a>.</p><table><thead><tr><th>Small pig and big pig</th><th>Press</th><th>Wait</th></tr></thead><tbody><tr><td>Press</td><td>1,5</td><td>-1,9</td></tr><tr><td>Wait</td><td>4,4</td><td>0,0</td></tr></tbody></table><p>In this experiment, we can see that for the small pig, there is a dominate strategy, so the small pig would prefer waiting. After eliminating pressing for small pig, the big pig would chose to press. However, for the big pig, there is no dominate strategy, but after iterative eliminating, (wait, press) becomes the nash equilibrium.</p><p>Someone may question about the relationship between Nash equilibrium and dominant strategy equilibrium. 优势策略均衡和纳什均衡的区别在于：在优势策略均衡中，我所做的是不管你做什么，我所能做的是最好的；在纳什均衡中，我所做的是给定你所做的前提下，我所能做的是最好的，你所做的是在给定我所做的前提下你所能做的是最好的，从二者的关系可以看出，优势策略均衡是纳什均衡的一个特例，一个优势策略均衡首先是一个纳什均衡.</p><h2 id="Maxmin-Strategies-and-Minmax-Strategies"><a href="#Maxmin-Strategies-and-Minmax-Strategies" class="headerlink" title="Maxmin Strategies and Minmax Strategies"></a>Maxmin Strategies and Minmax Strategies</h2><p>Maxmin strategy is a strategy that maximizes one’s worst-case payoff. Maxmin Value of the game for player i is that minimum payoff guaranteed by a maxmin strategy. A conservative agent would prefer the maxmin strategy.</p><p>Minmax strategy is a strategy that minimizes other’s worst-case payoff.</p><p>Theorem<br>: In any finite, two-player, zero-sum game, in any Nash equilibrium each player receives a payoff that is equal to both his maxmin value and his minmax value.</p><p>Note: in non-zero-sum game, Nash equilibrium may not equal to maxmin or minmax strategy.<a href="http://wiki.mbalib.com/zh-tw/%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E7%AD%96%E7%95%A5" target="_blank" rel="external">^1</a></p><h2 id="Corrleated-Equilibrium"><a href="#Corrleated-Equilibrium" class="headerlink" title="Corrleated Equilibrium"></a>Corrleated Equilibrium</h2><p>Correlated Equilibrium (informal): a randomized assignment of (potentially correlated) action recommendations to agents, such that nobody wants to deviate.</p><p>Reference: Correlated equilibrium<a href="https://en.wikipedia.org/wiki/Correlated_equilibrium" target="_blank" rel="external">^2</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This week’s material is dedicated to concepts “beyond Nash Equilibrium”: iterative removal of strictly dominated strategies, minimax stra
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="https://wattlebird.github.io/tags/Coursera/"/>
    
      <category term="Game theory" scheme="https://wattlebird.github.io/tags/Game-theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Note - week 2</title>
    <link href="https://wattlebird.github.io/2014/10/18/Game-Theory-Note-week-2/"/>
    <id>https://wattlebird.github.io/2014/10/18/Game-Theory-Note-week-2/</id>
    <published>2014-10-17T16:00:00.000Z</published>
    <updated>2017-10-02T01:46:40.424Z</updated>
    
    <content type="html"><![CDATA[<p>This week’s Game Theory is dedicated to Mixed-Strategy Nash Equilibrium.</p><p>Mixed strategy, different from pure strategy, means that players can choose an action according to a specific probability distribution (among all possible actions). The following concepts and definitions all derives from this idea:</p><p>Strategy $s_i$<br>: any probability distribution over the actions $A_i$ for agent i.</p><p>Pure strategy<br>: only one action is played with positive probability.</p><p>Mixed strategy<br>: more than one action is played with positive probability.</p><p>Support (of mixed strategy)<br>: all the actions</p><p>We denote $s_i \in S_i$ as $S_i$ is the set of all strategies for user i. All strategies $S = S_1 \times S_2 \times \ldots \times S_n$</p><p><strong>Expected Utility</strong> is defined as follows:<br><span>$$\begin{equation}u_{i}(s) = \sum_{a \in A} u_{i}(a) P(a|s) \P(a|s) = \prod_{j \in N} s_j(a_j)\end{equation}$$</span><!-- Has MathJax --><br>In the equations above, a means a possible action profile from A. $a_j$ does not mean each of the action but the player j’s corresponding action in the corresponding profile.</p><p>Best response</p><p>$s_{i}^{<em>} \in BR(s_{-i}) iff \forall s_i \in S_i u_{i}(s_{i}^{</em>}, s_{-i}) \ge u_{i}(s_i, s_{-i})$</p><p>Nash Equilibrium</p><p>$s=\<s\_1, s\_2,="" \ldots,="" s\_n="" \=""> \mbox( is a Nash Equilibrium iff }\forall i, s_i \in BR(s_{-i})$</s\_1,></p><p>Theorem<br>: Every finite game has a Nash Equilibrium. (While comparing to pure strategy games!)</p><p>It is often very hard to compute the Nash Equilibrium of a game, but in simple cases, in which we know the support, we can get the Nash Equilibrium by being acknowledged that a player will act indifferently facing a mixed strategy.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This week’s Game Theory is dedicated to Mixed-Strategy Nash Equilibrium.&lt;/p&gt;
&lt;p&gt;Mixed strategy, different from pure strategy, means that 
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="https://wattlebird.github.io/tags/Coursera/"/>
    
      <category term="Game theory" scheme="https://wattlebird.github.io/tags/Game-theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Note - week 1</title>
    <link href="https://wattlebird.github.io/2014/10/12/Game-Theory-Note-week-1/"/>
    <id>https://wattlebird.github.io/2014/10/12/Game-Theory-Note-week-1/</id>
    <published>2014-10-11T16:00:00.000Z</published>
    <updated>2015-05-01T20:00:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>This week’s game theory was dedicated to introduction, overview, uses of game theory, some applications and examples, and formal definitions of: the normal form, payoffs, strategies, pure strategy Nash equilibrium, dominant strategies..</p><h2 id="Define-a-Game"><a href="#Define-a-Game" class="headerlink" title="Define a Game"></a>Define a Game</h2><ol><li>Normal form: List what payoffs get as a function of their actions.</li><li>Extensive form: Includes timing of moves, players moves sequentially, represented as a tree.</li></ol><h3 id="Finite-n-person-normal-form-game"><a href="#Finite-n-person-normal-form-game" class="headerlink" title="Finite, n-person normal form game: $$:"></a>Finite, n-person normal form game: $<n, a,="" u="">$:</n,></h3><ul><li>Players: $N={1, \ldots, n}$, indexed by i;</li><li>Action set for player i: $a=(a_1, \ldots, a_n) \in A = A_1 \times \ldots \times A_n$ is an action profile;</li><li>Utility function or Payoff function for player i: $u_i:A \mapsto \mathbf{R} $, $u=(u_1, \ldots, u_n)$ is a profile of utility functions.</li></ul><h2 id="Type-of-Games"><a href="#Type-of-Games" class="headerlink" title="Type of Games"></a>Type of Games</h2><table><thead><tr><th>Type of Game</th><th>Properties</th><th>Examples</th></tr></thead><tbody><tr><td>Pure Competition</td><td>1. Exactly two players of opposed interests; Zero sum special case when $u_1(a)+u_2(a)=0$</td><td>Matching Pennies, Rock-Paper-Scissors</td></tr><tr><td>Coordination</td><td>Players have same interests: $\forall a \in A, \forall i,j, u_i(a)=u_j(a)$</td><td>side of road</td></tr><tr><td>Coordination and Competition</td><td></td><td>Battle of the Sexes</td></tr></tbody></table><h2 id="Nash-Equilibrium"><a href="#Nash-Equilibrium" class="headerlink" title="Nash Equilibrium"></a>Nash Equilibrium</h2><p>In game theory, the Nash equilibrium is a solution concept of a <strong>non-cooperative game</strong> involving two or more players, in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. If each player has chosen a strategy and no player can benefit by changing strategies while the other players keep theirs unchanged, then the current set of strategy choices and the corresponding payoffs constitute a Nash equilibrium.[^1]</p><p>Someone has an incentive to deviate from a profile of actions that do not form an equilibrium.</p><p>Best Resopnse<br>: If you knew what everyone else was going to do, it would be easy to pick your own action.<br>: Nash equilibrium looks for stable action profiles.</p><h2 id="Dominant-Strategies"><a href="#Dominant-Strategies" class="headerlink" title="Dominant Strategies"></a>Dominant Strategies</h2><p>Strategy (currently) is choosing an action (“pure strategy”)</p><p>Denote $s_i$ and $s_i’$ as two strategies for player i, and $S_{-i}$ be the set of all possible strategy profiles for the other players.</p><p>$s_i$ strictly dominates $s_i’$ if $ \forall s_{-i} \in S_{-i}, u_{i}(s_i, s_{-i}) \gt u_{i}(s_i’, s_{-i})$<br>$s_i$ very weakly dominates $s_i’$ if $ \forall s_{-i} \in S_{-i}, u_{i}(s_i, s_{-i} ) \ge u_{i}(s_i’, s_{-i})$<br>Please pay attention to the difference between best response, which lies in the definition of strategy.</p><p>A strategy profile consisting of dominant strategies for every player must be a Nash equilibrium! An equilibrium in strictly dominant strategies must be unique.</p><h2 id="Pareto-Optimality"><a href="#Pareto-Optimality" class="headerlink" title="Pareto Optimality"></a>Pareto Optimality</h2><p>Some times, one outcome $o$ is at least as good for every agent as another outcome $o’$, and there’s some agent who strictly prefers $o$ to $o’$.</p><p>An outcome $o^*$ is Pareto-optimal if there is no other outcome that  Pareto-dominates it.</p><p>[^1]: Nash equilibrium, <a href="https://en.wikipedia.org/wiki/Real_number" target="_blank" rel="external">https://en.wikipedia.org/wiki/Real_number</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This week’s game theory was dedicated to introduction, overview, uses of game theory, some applications and examples, and formal definiti
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="https://wattlebird.github.io/tags/Coursera/"/>
    
      <category term="Game theory" scheme="https://wattlebird.github.io/tags/Game-theory/"/>
    
  </entry>
  
  <entry>
    <title>一种树的表达法</title>
    <link href="https://wattlebird.github.io/2014/10/04/%E4%B8%80%E7%A7%8D%E6%A0%91%E7%9A%84%E8%A1%A8%E8%BE%BE%E6%B3%95/"/>
    <id>https://wattlebird.github.io/2014/10/04/一种树的表达法/</id>
    <published>2014-10-03T16:00:00.000Z</published>
    <updated>2015-05-01T21:28:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>《算法导论》教导我们，图有两种表达方法：邻接链表和邻接矩阵。至少是对于有环的图来说，的确是这样。对于树和图究竟怎样表达最好，我一直都在摸索之中。以前在写与图有关的算法的时候，总是要把整个邻接链表（通常是一个很大的<code>std::vector&lt;std::list&lt;T&gt;&gt;&amp;</code>）当作参数的一部分送进去。这样看着就很别扭，但是这确实是严格按照《算法导论》的定义实现的。</p><p>因此我对我的代码质量深表担忧。但是幸运的是，最近我开始学习起别人的代码，从中获得了一些启示。今天总结的是这么一种特殊的图————自由树的表达方法。这种方法高效，无论是从时间上还是空间上。</p><p>假定现在给出一系列树边，每行一条树边记录，由两个数组成。第一个数代表父结点，第二个数代表子结点。这一系列树边可以组成一棵树（或一片森林）。对于这样的信息，利用下面这种方法可以给出高效的表达。</p><p>首先我们定义如下结构和数组：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></div><div class="line">    <span class="keyword">int</span> v;</div><div class="line">    <span class="keyword">int</span> prev;</div><div class="line">&#125;<span class="keyword">edge_t</span>;</div><div class="line"></div><div class="line"><span class="keyword">edge_t</span> edges[N]; <span class="comment">//边</span></div><div class="line"><span class="keyword">int</span> head[N];     <span class="comment">//结点</span></div></pre></td></tr></table></figure><p>我们知道，在一棵结点数为V的树中，一共有V-1条边。因此给结点和边各开V大小的数组是没问题的。利用这样的数据结构，我们使用如下方法读取边：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//for each edge (u,v)</span></div><div class="line">edges[i].v=v;</div><div class="line">edges[i].prev=head[u];</div><div class="line">head[u]=i;</div></pre></td></tr></table></figure><p>从这段代码中，我们可以看出如下基本事实：</p><ol><li>edges以边序号为索引，其中v记录的是该边的子结点，prev记录的是另一条边序号；</li><li>head以结点序号为索引，其中记录的是边序号；</li><li>在第一次涉及到父结点u时，head[u]为0，此时将此值赋给prev，代表空边。在这之后，head[u]被赋予本边序号；</li><li>如果再次遇到父结点u，head[u]会被新的边序号覆盖。</li><li>因此，head实际上存储着以某个结点为父结点时，其在读取顺序中的最后一条边。如果无子结点，则为0.</li></ol><p>那么，每个结点最多只存储一条边，如何实现多个子结点的情况？事实上，我们在读到这“最后一条边”时，依照读取顺序的上一条边已经被存储在edges[i].prev中了。按照如下代码，可以方便地完成对某个结点所有子结点的遍历：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k=head[i]; k!=<span class="number">0</span>; k=edges[k].prev)&#123;</div><div class="line">    <span class="comment">//do something</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这种方法适用于：</p><ul><li>数据以自由树形式表达；</li><li>输入数据是边的信息，且标明了父亲孩子关系。</li><li>输入边的顺序可以任意。如果要确定根结点，只要再加上一个<code>bool isroot[V]</code>就可以在读取的时候把所有子结点标注出来。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;《算法导论》教导我们，图有两种表达方法：邻接链表和邻接矩阵。至少是对于有环的图来说，的确是这样。对于树和图究竟怎样表达最好，我一直都在摸索之中。以前在写与图有关的算法的时候，总是要把整个邻接链表（通常是一个很大的&lt;code&gt;std::vector&amp;lt;std::list&amp;
      
    
    </summary>
    
      <category term="OJ Review" scheme="https://wattlebird.github.io/categories/OJ-Review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
      <category term="Data Structure" scheme="https://wattlebird.github.io/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>树的直径</title>
    <link href="https://wattlebird.github.io/2014/09/21/%E6%A0%91%E7%9A%84%E7%9B%B4%E5%BE%84/"/>
    <id>https://wattlebird.github.io/2014/09/21/树的直径/</id>
    <published>2014-09-20T16:00:00.000Z</published>
    <updated>2015-05-01T21:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>求一个自由树的直径。对于直径，《算法导论》第三版 349 页练习 22.2-8 上面这么定义道：</p><blockquote><p>树中所有最短路径的最大值即为树的直径。</p></blockquote><p>这个树由于没有根结点，其实直径这个概念，还是理解为一个连通无向无环图的直径为好。</p><p>现在给定如下格式的输入：</p><p>8<br>1 2<br>1 3<br>1 4<br>4 5<br>3 6<br>6 7<br>7 8  </p><p>第一行是这个图的结点个数，不妨记为 N，以下 N-1 行是 N-1 条边，结点序号按照 1-N 顺序编号。求这个图的直径。</p><p>这个输入的输出结果是 6，给出这个示意图就可以看得很清楚：<br><img src="http://media.hihocoder.com/problem_images/20140913/14105773975774.png" alt=""></p><p>对于这个问题，最笨的方法就是对每一个结点进行 BFS，因为 BFS 有这个性质：BFS 生成的 广度优先树的每一个结点到达根结点的路径总是最短路。这样，把每一个结点 BFS 一遍就会生成一个该结点到达的最远结点。按照定义取出最长的路径即可。由于 BFS 时间复杂度是 O(N)，这个方法的时间复杂度是$O(N^2)$。</p><p>其实还有一个更为简便的方法：首先对任意一个结点做 BFS 求出最远的结点，然后以这个结点为根结点再做 BFS 到达另一个最远结点。第一次 BFS 到达的结点可以证明一定是这个图的直径的一端，第二次 BFS 就会达到另一端。下面来证明这个定理。</p><p>但是在证明定义之前，先证明一个引理：</p><p><strong>引理</strong>：在一个连通无向无环图中，x、y 和 z 是三个不同的结点。当 x 到 y 的最短路与 y 到 z 的最短路不重合时，x 到 z 的最短路就是这两条最短路的拼接。</p><p><strong>证明</strong>：假设 x 到 z 有一条不经过 y 的更短路$\delta (x,z)$，则该路与$\delta (x,y)$、$\delta (y,z)$形成一个环，与前提矛盾。</p><p><strong>定理</strong>：在一个连通无向无环图中，以任意结点出发所能到达的最远结点，一定是该图直径的端点之一。</p><p><strong>证明</strong>：假设这条直径是$\delta (s,t)$。分两种情况：</p><ol><li>当出发结点 y 在$\delta (s,t)$时，假设到达的最远结点 z 不是 s,t 中的任一个。这时将$\delta (y,z)$与不与之重合的$\delta (y,s)$拼接（也可以假设不与之重合的是直径的另一个方向），可以得到一条更长的直径，与前提矛盾。</li><li><p>当出发结点 y 不在$\delta (s,t)$上时，分两种情况：<br>1). 当 y 到达的最远结点 z 横穿$\delta (s,t)$时，记与之相交的结点为 x。此时有$\delta (y,z)=\delta (y,x)+\delta (x,z)$。而此时$\delta (y,z)&gt;\delta (y,t)$，故可得$\delta (x,z)&gt;\delta (x,t)$。由1的结论可知该假设不成立。<br><img src="/img/tree-illu1.png" alt=""></p><p> 2). 当 y 到达的最远结点 z 与$\delta (s,t)$不相交时，记 y 到 t 的最短路首先与$\delta (s,t)$相交的结点是 x。由假设$\delta (y,z)&gt;\delta (y,x)+\delta (x,t)$。而$\delta (y,z)+\delta (y,x)+\delta (x,s)$又可以形成$\delta (z,s)$，而$\delta (z,s)&gt;\delta (x,s)+\delta (x,t)+2\delta (y,x)=\delta (s,t)+2\delta(y,x)$，显然与题意矛盾。<br><img src="/img/tree-illu2.png" alt=""></p></li></ol><p>因此定理成立。</p><p><strong>9月21日补充</strong>：这道题是上一周 hihocoder 上面的一道题。出题者的原意并不是要我们这么做。出题者写了很长的一段提示，但是这段提示的语文表述很差，完全没有抓住重点，导致我花了一个星期的时间也没弄明白他在讲什么。现在所有人的源代码均已公开，可以继续下去了。</p><p>出题者的原意是要我们使用这么一个定理：</p><p><strong>定理2</strong>：树的直径，等于以树直径上任意一点为根的有根树，其左子树的高度+1，再加上其右子树高度+1。</p><p>按照这种定理的定义，我们可以设计这样一个程序，对每个结点计算左子树高度+右子树高度+2.这样的时间复杂度是$O(n^2)$。由于我们不知道所选取的结点是否是在直径上，所以要进行这样的枚举。显然这会超时。但是根据<a href="http://www.geeksforgeeks.org/diameter-of-a-binary-tree/" target="_blank" rel="external">本文</a>的提示，寻找这种直径的过程其实可以递归化：</p><ol><li>在根结点的左子树上；</li><li>在根结点的右子树上；</li><li>直径经过根结点。</li></ol><p>于是我们可以设计这样的程序：选取任意结点为根结点，递归地计算每个结点的高度。在结点内部计算高度的同时，计算以当前结点为根的子树的左子树高度+右子树高度+2，用于更新全局树直径。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;求一个自由树的直径。对于直径，《算法导论》第三版 349 页练习 22.2-8 上面这么定义道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;树中所有最短路径的最大值即为树的直径。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个树由于没有根结点，其实直径这个概念，还是理解为一个
      
    
    </summary>
    
      <category term="OJ Review" scheme="https://wattlebird.github.io/categories/OJ-Review/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
      <category term="Data Structure" scheme="https://wattlebird.github.io/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>二叉搜索树与快速排序的内在相似性</title>
    <link href="https://wattlebird.github.io/2014/07/27/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E7%9A%84%E5%86%85%E5%9C%A8%E7%9B%B8%E4%BC%BC%E6%80%A7/"/>
    <id>https://wattlebird.github.io/2014/07/27/二叉搜索树与快速排序的内在相似性/</id>
    <published>2014-07-26T16:00:00.000Z</published>
    <updated>2017-10-02T01:53:56.201Z</updated>
    
    <content type="html"><![CDATA[<p>对我来说，对随机事件的分析，恐怕是最难的。我原以为我数学学得还可以，直到我遇上了随机过程。这篇 blog 所讲的是算法分析，其中涉及到大量对随机情况的分析。因此我在此将其梳理一下，特别注重挖掘不同算法之间的分析过程的相似性。</p><p>快速排序是一种原址排序方法，随机化的快排具有 <em>O(nlgn)</em> 的期望运行时间。这个在《算法导论》（第三版）的 7.4.2 节中有一个以比较操作为中心的证明方法。这个证明的核心思想就是：快速排序是由多次 partition 过程组成的，因此关键问题就在于获得 partition 过程的运行时间和运行 partition 过程的次数。partition 过程的最大运行次数是 n-1 次，可以记为 <em>O(n)</em> ，对于其运行时间，练习 7.1-3 给出的结论是 <em>Θ(n)</em> 。这样乍一算，似乎是 <em>O($n^2$)</em> 的时间复杂度。这样计算是错误的，因为 partition 的时间复杂度与其长度有关。这里，我们需要更为细致的分析。决定 partition 运行时间的是其内部循环次数。这个循环次数可以统计每次必须运行的比较次数而得到。通过计算整个 quicksort 的比较次数，我们就可以得到真正的 partition 循环次数。而这个比较次数的期望值，可以通过拆分成示性随机变量相加得到。这个分析的精华之处就在于分析出每两个元素进行比较的概率。很有意思，可以进行比较的组合是 <em>Θ($n^2$)</em> ，但是最终全部比较次数的期望是 <em>O(n lgn)</em> 。</p><p>当然这个只是作为复习，不是今天的重点。这个方法实在是太不直观了点。这次我们用类似于 merge-sort 的分析方法进行分析。</p><h3 id="快速排序分析"><a href="#快速排序分析" class="headerlink" title="快速排序分析"></a>快速排序分析</h3><p>按照 merge-sort 的分析思路，quicksort 是将一个问题拆分成了两个子问题，但是由于子问题大小不是固定的，这时候就只能分析运行时间的期望。随机化的快速排序使得任何一个元素成为主元都是等可能的。因此我们有如下式子：</p><span>$$\begin{aligned}E[T(n)] &amp; =  E \left[ \sum^{n}_{q=1} X_q \left( T(q-1) + T(n-q) + \Theta (n) \right) \right] \\            &amp; =  \frac{2}{n} \sum^{n-1}_{q=2} E[T(q)] + \Theta (n)\end{aligned}$$</span><!-- Has MathJax --><p>随后，可以通过代入法，把 <em>n lgn</em> 代入 T(n)。其中，有如下不等式可以利用：</p><span>$$\begin{equation}\sum^{n-1}_{k=2} k \log k \le \frac{1}{2} n^2 \log n - \frac{1}{8} n^2\end{equation}$$</span><!-- Has MathJax --><p>由于这个不等式的天赐特性，我们只能记住，有如（1）式的结论就是 <em>O(n lgn)</em> 。</p><h3 id="二叉搜索树分析"><a href="#二叉搜索树分析" class="headerlink" title="二叉搜索树分析"></a>二叉搜索树分析</h3><p>我们知道，二叉搜索树的动态操作时间复杂度是 <em>O(h)</em> 。但是对于随机构建的二叉搜索树来说，其期望树高是 <em>O(n lgn)</em> ，对于随机构建的二叉搜索树来说。这里我们证明的是一个稍弱于此定理的定理：<strong>随机构建的二叉搜索树的平均节点深度为 <em>O(n lgn)</em></strong>。</p><p>为表示每一个节点的深度，我们记树<strong><em>T</em></strong>的节点x的深度为 <em>d(x, T)</em> ，而全部节点的深度之和记为 <em>P(T)</em> 。节点平均深度可以表示为</p><span>$$\begin{equation}\frac{1}{n} \sum_{x \in T} d(x, T) = \frac{1}{n} P(T)\end{equation}$$</span><!-- Has MathJax --><p>而每一棵树可以拆分为节点与左子树、右子树。我们需要注意，当把 P(T) 拆分为$P(T<em>{left})$和$P(T</em>{right})$之后深度还应该增加当前树总节点再减一。也就是</p><span>$$\begin{equation}P(T)=P(T_{left})+P(T_{right})+n-1\end{equation}$$</span><!-- Has MathJax --><p>对于某一棵树确实是这样。但是这棵树是随机构建的。如何表示出 P(T) 的期望值？事实上，这里和快速排序一样，在随机构建的过程中，第一个元素总是根节点，每一个元素成为第一个元素的概率都是相等的。因此，我们可以据此写出：</p><span>$$\begin{equation}E[P(n)]=E\left[ \frac{1}{n} \sum_{k=0}^{n-1}(P(k)+P(n-k-1)+n-1)\right]\end{equation}$$</span><!-- Has MathJax --><p>其中 P(n) 是具有 n 个节点的树高。这时候，我们发现，这和在快速排序那里推导出来的式子是非常相似的。因此，延续着快排分析的思路，可以分析出 P(n) = O(n lgn)。</p><p>知道这一点有什么用呢？当构建一棵二叉搜索树时，第一个元素会被选为根节点，其后的元素，每一个都要和其比较。这和快速排序的比较次数是一样的。因为当一个元素选为主元的时候，其后的每一个元素都要和其比较。这样，当用相同的序列构建二叉搜索树和进行快速排序的时候，他们所需要的比较次数是相同的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对我来说，对随机事件的分析，恐怕是最难的。我原以为我数学学得还可以，直到我遇上了随机过程。这篇 blog 所讲的是算法分析，其中涉及到大量对随机情况的分析。因此我在此将其梳理一下，特别注重挖掘不同算法之间的分析过程的相似性。&lt;/p&gt;
&lt;p&gt;快速排序是一种原址排序方法，随机化
      
    
    </summary>
    
      <category term="Analytical" scheme="https://wattlebird.github.io/categories/Analytical/"/>
    
    
      <category term="Algorithm" scheme="https://wattlebird.github.io/tags/Algorithm/"/>
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
      <category term="Data Structure" scheme="https://wattlebird.github.io/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>简单易懂的XML parsing——Qt篇</title>
    <link href="https://wattlebird.github.io/2014/05/30/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E7%9A%84XML-parsing%E2%80%94%E2%80%94Qt%E7%AF%87/"/>
    <id>https://wattlebird.github.io/2014/05/30/简单易懂的XML-parsing——Qt篇/</id>
    <published>2014-05-29T16:00:00.000Z</published>
    <updated>2015-05-01T21:22:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>关于XML的文章我先前写过一篇。之后就再也没写过。原因是很简单的。虽然那篇文章是用Matlab的代码说明XML解析，但是XML的基本概念都是一致的，我也没必要再就C++或是Python等语言再写一遍在其他语言下面怎么用其他的库解析XML，都是大同小异。</p><p>可是这个世界上奇葩比较多。最近在做《网络通信原理》的project的时候，用到了Qt里面的QXmlStreamReader。有意思的是，这个东西不按常理出牌。为说明这个特性，我引用Qt 5关于QXmlStreamReader上面的一段话：</p><blockquote><p>QXmlStreamReader is an incremental parser. It can handle the case where the document can’t be parsed all at once because it arrives in chunks (e.g. from multiple files, or over a network connection).<br>…<br>QXmlStreamReader is memory-conservative by design, since it doesn’t store the entire XML document tree in memory, but only the current token at the time it is reported.</p></blockquote><p>由这段话我们可以看出，QXmlStreamReader的一个重要特点是，它是一个增量parser。QXmlStreamReader有一个特别的构造函数<code>QXmlStreamReader::QXmlStreamReader(QIODevice * device)</code>，这个device可以是QNetworkReply也可以是QFile。相信这样的好处大家都可以看得出来。为了应付不同IODevice的特性，QXmlStreamReader也只能采取增量解析的方法。然后又有了下面的概念：token.</p><p>QXmlStreamReader不在内存中保存全部的DOM tree，现在解析的位置和所解析的对象用token说明。关于什么是token，其实我也不知道。但是QXmlStreamReader提供了一个函数：<code>TokenType QXmlStreamReader::readNext()</code>，有关这个函数的说明是“Reads the next token and returns its type.”</p><p>按照官方文档上面的解释，一个可行的解析模型可以是这样：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">QXmlStreamReader xml;</div><div class="line">...</div><div class="line"><span class="keyword">while</span> (!xml.atEnd()) &#123;</div><div class="line">      xml.readNext();</div><div class="line">      ... <span class="comment">// do processing</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (xml.hasError()) &#123;</div><div class="line">      ... <span class="comment">// do error handling</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>由此可见，QXmlStreamReader在解析xml的时候，以token为单位解析xml文档数据。</p><p>我在上一篇文章中讲过，xml有Element node，Element node以Text node作为child，Attribute node从属于Element node，comment node相对独立，而以上四种node都由document node生成，document node可以说是一个xml文档的代表，xml parsing的核心是element node。但是在Qt中，token与这种标准的概念似乎完全无关。它更关心我现在读到的东西是什么。在TokenType的定义中，一共给出了9种不同token的定义，而判断当前parser的tokenType是什么的函数一共有十二种。</p><p>我们可以想象，这种parser，一块一块地读取xml文档，只前进不后退，每一块代表一种既定的token，直到全部读完xml为止（也就是<code>atEnd()</code>为真的时候）。</p><p>下面让我展示一段我这个project中的一段代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>(reply-&gt;error()==QNetworkReply::NoError)&#123;</div><div class="line">    ui-&gt;listWidget-&gt;clear();</div><div class="line">    articlelist.clear();</div><div class="line">    <span class="function">QXmlStreamReader <span class="title">xml</span><span class="params">(reply)</span></span>;</div><div class="line">    <span class="keyword">if</span>(xml.readNextStartElement() &amp;&amp; xml.name()==<span class="string">"articles"</span>)&#123;</div><div class="line">        <span class="keyword">while</span>(xml.readNextStartElement() &amp;&amp; xml.name()==<span class="string">"article"</span>)&#123;</div><div class="line">            Article record;</div><div class="line">            <span class="keyword">while</span>(xml.readNextStartElement())&#123;</div><div class="line">                <span class="keyword">if</span>(xml.name()==<span class="string">"author"</span>)&#123;</div><div class="line">                    record.author = xml.readElementText();</div><div class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(xml.name()==<span class="string">"date"</span>)&#123;</div><div class="line">                    record.date = xml.readElementText();</div><div class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(xml.name()==<span class="string">"title"</span>)&#123;</div><div class="line">                    QString t = xml.readElementText();</div><div class="line">                    ui-&gt;listWidget-&gt;addItem(t);</div><div class="line">                    record.title = t;</div><div class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(xml.name()==<span class="string">"content"</span>)&#123;</div><div class="line">                    record.content = xml.readElementText();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            articlelist.push_back(record);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>xml文档格式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">articles</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">article</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">author</span>&gt;</span>...<span class="tag">&lt;/<span class="name">author</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">date</span>&gt;</span>...<span class="tag">&lt;/<span class="name">date</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>...<span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">content</span>&gt;</span>...<span class="tag">&lt;/<span class="name">content</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">article</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">article</span>&gt;</span></div><div class="line">  ...</div><div class="line">  <span class="tag">&lt;/<span class="name">article</span>&gt;</span></div><div class="line">  ...</div><div class="line"><span class="tag">&lt;/<span class="name">articles</span>&gt;</span></div></pre></td></tr></table></figure><p>代码中reply是个API请求的回应，我的目的是吧这个回应中的每一条信息存放在articlelist中。值得注意的是14-16行那段代码，由于这个是一个增量parser，我们不能使用</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ui-&gt;listWidget-&gt;addItem(xml.readElementText());</div><div class="line">record.title = xml.readElementText();</div></pre></td></tr></table></figure><p>否则<code>record.title</code>将为空。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于XML的文章我先前写过一篇。之后就再也没写过。原因是很简单的。虽然那篇文章是用Matlab的代码说明XML解析，但是XML的基本概念都是一致的，我也没必要再就C++或是Python等语言再写一遍在其他语言下面怎么用其他的库解析XML，都是大同小异。&lt;/p&gt;
&lt;p&gt;可是这
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="C++" scheme="https://wattlebird.github.io/tags/C/"/>
    
      <category term="xml" scheme="https://wattlebird.github.io/tags/xml/"/>
    
  </entry>
  
  <entry>
    <title>简单易懂的XML parsing</title>
    <link href="https://wattlebird.github.io/2014/03/11/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E7%9A%84XML-parsing/"/>
    <id>https://wattlebird.github.io/2014/03/11/简单易懂的XML-parsing/</id>
    <published>2014-03-10T16:00:00.000Z</published>
    <updated>2015-05-01T21:18:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>读取一个XML文件，返回一个DOM对象。</p><p>什么是DOM对象？全称为Document Object Model, XML文件中的每一个东西都对应为一个node。DOM node的属性和方法遵循国际互联网的标准。</p><p>有以下类型的nodes：</p><ul><li>Element nodes*   Text nodes 每一个Text node都是Element node的child</li><li>Attribute nodes 不是任何node的parent 或 child,从属于element node</li><li>Comment nodes*   Document nodes 只有使用document node的方法才能创造新element, text, attribute, comment</li></ul><p>现有以下xml文档：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">listitem</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">label</span>&gt;</span>Import Wizard<span class="tag">&lt;/<span class="name">label</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">callback</span>&gt;</span>uiimport<span class="tag">&lt;/<span class="name">callback</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">icon</span>&gt;</span>ApplicationIcon.GENERIC_GUI<span class="tag">&lt;/<span class="name">icon</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">listitem</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">listitem</span>&gt;</span></div><div class="line">    ...</div><div class="line"><span class="tag">&lt;/<span class="name">listitem</span>&gt;</span></div><div class="line">    ...</div></pre></td></tr></table></figure><p>其中的一个label标签有字符Plot Tools。假设你想在同样的listitem里面寻找callback标签的字符：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">findLabel = <span class="string">'Plot Tools'</span>;</div><div class="line">findCbk = <span class="string">''</span>;</div><div class="line">xDoc = xmlread(fullfile(matlabroot, ...</div><div class="line">               <span class="string">'toolbox'</span>,<span class="string">'matlab'</span>,<span class="string">'general'</span>,<span class="string">'info.xml'</span>));</div><div class="line">allListitems = xDoc.getElementsByTagName(<span class="string">'listitem'</span>);</div><div class="line"><span class="keyword">for</span> k = <span class="number">0</span>:allListitems.getLength<span class="number">-1</span></div><div class="line">    thisListitem = allListitems.item(k);</div><div class="line"></div><div class="line">    <span class="comment">% Get the label element. In this file, each</span></div><div class="line">    <span class="comment">% listitem contains only one label.</span></div><div class="line">    thisList = thisListitem.getElementsByTagName(<span class="string">'label'</span>);</div><div class="line">    thisElement = thisList.item(<span class="number">0</span>);</div><div class="line">    <span class="comment">% Check whether this is the label you want.</span></div><div class="line">    <span class="comment">% The text is in the first child node.</span></div><div class="line">    <span class="keyword">if</span> strcmp(thisElement.getFirstChild.getData, findLabel)</div><div class="line">        thisList = thisListitem.getElementsByTagName(<span class="string">'callback'</span>);</div><div class="line">        thisElement = thisList.item(<span class="number">0</span>);</div><div class="line">        findCbk = char(thisElement.getFirstChild.getData);</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> ~<span class="built_in">isempty</span>(findCbk)</div><div class="line">    msg = sprintf(<span class="string">'Item "%s" has a callback of "%s."'</span>,...</div><div class="line">                      findLabel, findCbk);</div><div class="line"><span class="keyword">else</span></div><div class="line">   msg = sprintf(<span class="string">'Did not find the "%s" item.'</span>, findLabel);</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="built_in">disp</span>(msg);</div></pre></td></tr></table></figure><p>MATLAB本身就提供一个xmlread函数，其返回的是Document Node。根节点哦。其余与Document Node的函数都是标准已经定义了的，这个标准详情请见<a href="http://download.oracle.com/javase/6/docs/api/" target="_blank" rel="external">这里</a>.在上面一段代码中，我们可以看见几个常用的API：</p><ul><li><code>getElementsByTagName</code>是Document Node的方法，返回一个list。</li><li>这个list是node的列表，要得到其中一个元素，需要调用list的<code>item</code>方法。</li><li>一个有child的element node，获得其内容，要调用<code>getFirstChild.getData</code>。</li></ul><p>如果我们想写一个XML文档：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span><span class="meta">?&gt;</span></span></div><div class="line"><span class="tag">&lt;<span class="name">toc</span> <span class="attr">version</span>=<span class="string">"2.0"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">tocitem</span> <span class="attr">target</span>=<span class="string">"upslope_product_page.html"</span>&gt;</span>Upslope Area Toolbox<span class="comment">&lt;!-- Functions --&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">tocitem</span> <span class="attr">target</span>=<span class="string">"demFlow_help.html"</span>&gt;</span>demFlow<span class="tag">&lt;/<span class="name">tocitem</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">tocitem</span> <span class="attr">target</span>=<span class="string">"facetFlow_help.html"</span>&gt;</span>facetFlow<span class="tag">&lt;/<span class="name">tocitem</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">tocitem</span> <span class="attr">target</span>=<span class="string">"flowMatrix_help.html"</span>&gt;</span>flowMatrix<span class="tag">&lt;/<span class="name">tocitem</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">tocitem</span> <span class="attr">target</span>=<span class="string">"pixelFlow_help.html"</span>&gt;</span>pixelFlow<span class="tag">&lt;/<span class="name">tocitem</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">tocitem</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">toc</span>&gt;</span></div></pre></td></tr></table></figure><p>MATLAB代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">docNode = com.mathworks.xml.XMLUtils.createDocument(<span class="string">'toc'</span>);</div><div class="line"></div><div class="line">toc = docNode.getDocumentElement;</div><div class="line">toc.setAttribute(<span class="string">'version'</span>,<span class="string">'2.0'</span>);</div><div class="line"></div><div class="line">product = docNode.createElement(<span class="string">'tocitem'</span>);</div><div class="line">product.setAttribute(<span class="string">'target'</span>,<span class="string">'upslope_product_page.html'</span>);</div><div class="line">product.appendChild(docNode.createTextNode(<span class="string">'Upslope Area Toolbox'</span>));</div><div class="line">toc.appendChild(product)</div><div class="line"></div><div class="line">product.appendChild(docNode.createComment(<span class="string">' Functions '</span>));</div><div class="line"></div><div class="line">functions = &#123;<span class="string">'demFlow'</span>,<span class="string">'facetFlow'</span>,<span class="string">'flowMatrix'</span>,<span class="string">'pixelFlow'</span>&#125;;</div><div class="line"><span class="keyword">for</span> idx = <span class="number">1</span>:<span class="built_in">numel</span>(functions)</div><div class="line">    curr_node = docNode.createElement(<span class="string">'tocitem'</span>);</div><div class="line"></div><div class="line">    curr_file = [functions&#123;idx&#125; <span class="string">'_help.html'</span>];</div><div class="line">    curr_node.setAttribute(<span class="string">'target'</span>,curr_file);</div><div class="line"></div><div class="line">    <span class="comment">% Child text is the function name.</span></div><div class="line">    curr_node.appendChild(docNode.createTextNode(functions&#123;idx&#125;));</div><div class="line">    product.appendChild(curr_node);</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">xmlwrite(<span class="string">'info.xml'</span>,docNode);</div><div class="line">type(<span class="string">'info.xml'</span>);</div></pre></td></tr></table></figure><ul><li>这个函数首先先创建出一个Document Node，也就是最重要的根节点；</li><li><code>SetAttribute</code>是Element Node的方法；</li><li>element, text, attribute, comment只能由docNode创建，方法是<code>createXXXNode</code>；</li><li>Element Node间的父子关系由<code>appendChild</code>确定。</li></ul><p>以上就是MATLAB里面处理XML文档的最基本知识。由于XML文档的处理方式是统一的，因此很容易就能拓展到其他语言。从代码中就可以挖掘出许多东西。在实际中，要使用到的XML API恐怕还远远不够。这篇文章只是作为一个入门性质的导引。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;读取一个XML文件，返回一个DOM对象。&lt;/p&gt;
&lt;p&gt;什么是DOM对象？全称为Document Object Model, XML文件中的每一个东西都对应为一个node。DOM node的属性和方法遵循国际互联网的标准。&lt;/p&gt;
&lt;p&gt;有以下类型的nodes：&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="Notes" scheme="https://wattlebird.github.io/categories/Notes/"/>
    
    
      <category term="xml" scheme="https://wattlebird.github.io/tags/xml/"/>
    
      <category term="matlab" scheme="https://wattlebird.github.io/tags/matlab/"/>
    
  </entry>
  
</feed>
