<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Mangaki data challenge 1st place solution | Dedication</title>
  <meta name="author" content="Yet another tech blog!">
  
  <meta name="description" content="Mangaki data challenge is an otaku-flavor oriented data science competition. It’s goal is to predict user’s preference of an unwatched/unread anime/ma">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Mangaki data challenge 1st place solution"/>
  <meta property="og:site_name" content="Dedication"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/atom.xml" title="Dedication" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/paper.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-53608123-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

 <body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar  navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Dedication</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/links" title="Friend links.">
			  <i class="fa fa-link"></i>Links
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header ">		
			<h1 class="title "> Mangaki data challenge 1st place solution</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p><a href="http://universityofbigdata.net/competition/5085548788056064?lang=en" target="_blank" rel="external">Mangaki data challenge</a> is an otaku-flavor oriented data science competition. It’s goal is to predict user’s preference of an unwatched/unread anime/manga from two choices: wish to watch/read and don’t want to watch/read. This competition provides training data from <a href="https://mangaki.fr/" target="_blank" rel="external">https://mangaki.fr/</a> which allows users to favorite their anime/manga works. Three major training tables are provided as described as follows:</p>
<ol>
<li>Wish table: about 10k rows</li>
</ol>
<table>
<thead>
<tr>
<th>User_id</th>
<th>Work_id</th>
<th>Wish</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>233</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<ol>
<li>Record table: for already watched/read anime/manga. There are four rates here: love, like, neutral and dislike.</li>
</ol>
<table>
<thead>
<tr>
<th>User_id</th>
<th>Work_id</th>
<th>Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>22</td>
<td>like</td>
</tr>
<tr>
<td>2</td>
<td>33</td>
<td>dislike</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<ol>
<li>Work table: detailed information of available anime/manga. There are three categories: anime, manga and album. There is only one album in this table, all the others are anime (about 7k) and manga (about 2k)</li>
</ol>
<table>
<thead>
<tr>
<th>Work_id</th>
<th>Title</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Some_anime</td>
<td>anime</td>
</tr>
<tr>
<td>1</td>
<td>Some_manga</td>
<td>manga</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>For the testing data, one should predict 100k user/work pair on whether the user wish or not wish to watch/read an anime/manga. As you can see, the testing data is much larger than training data. Besides, during my analysis of this dataset, it is also not ensured that all users or works appeared in test set are contained in training set. </p>
<h2 id="Traditional-recommendation-system-methods-that-I-know"><a href="#Traditional-recommendation-system-methods-that-I-know" class="headerlink" title="Traditional recommendation system methods (that I know)"></a>Traditional recommendation system methods (that I know)</h2><p>Recommendation system building has long been studied and there are various methods in solving this particular problem. For me, I also tried to build a recommender for <a href="https://bgm.tv" target="_blank" rel="external">https://bgm.tv</a> several years ago (you can read technical details <a href="https://wattlebird.github.io/2015/03/08/How-I-build-up-Chi-%E5%90%8C%E6%AD%A5%E7%8E%87%E6%94%B9-v0-3-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/">here</a>). The simplest solution is SVD (actually, a more simple and intuitive solution is by using KNN), then one can move on to RBM, FM, FFM and so on. One assumption that holds firm in all these methods is that users should have an embedding vector capturing their preferences, and works should also have their embedding vector capturing their characteristics. It is reasonable that we should be constrained in this embedding-dotproduct model?</p>
<p>Recently, the common practice on Kaggle competition is by using GBDT to solve (almost all except computer vision related) questions. As long as a model can handle classification, regression and ranking problem very well, it can be applied in all supervised machine learning problems! And by using model ensembing under stacknet framework, one can join different characteristics of models altogether to achieve the best result.</p>
<p>In this competition, my solution is quite fair and straightforward: feature engineering to generate some embeddings, and use GBDT/Random Forest/Factorization Machine to build models from different combinations of features. After all, I used a two-level stack net to ensemble them, in which level two is a logistic regression model.</p>
<h2 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h2><h3 id="From-wish-table"><a href="#From-wish-table" class="headerlink" title="From wish table:"></a>From wish table:</h3><ul>
<li>Distribution of user’s preference on anime/manga (2d+2d)</li>
<li>Distribution of item’s preference (2d)</li>
<li>Word2vec embedding of user on wish-to-watch items (20d)</li>
<li>Word2vec embedding of user on not-wish-to-watch items (10d)</li>
<li>Word2vec embedding of item on wish-to-watch users (20d)</li>
<li>Word2vec embedding of item on not-wish-to-watch users (10d)</li>
<li>Lsi embedding of user (20d)</li>
<li>Lsi embedding of item (20d)</li>
</ul>
<h3 id="From-record-table"><a href="#From-record-table" class="headerlink" title="From record table:"></a>From record table:</h3><ul>
<li>Distribution of user’s preference on anime/manga (4d+4d)</li>
<li>Distribution of item’s preference (4d)</li>
<li>Mean/StdErr of user’s rating (2d)</li>
<li>Mean/StdErr of item’s rating (2d)</li>
<li>Word2vec embedding of user on wish-to-watch items (32d)</li>
<li>Word2vec embedding of user on not-wish-to-watch items (10d)</li>
<li>Word2vec embedding of item on wish-to-watch users (32d)</li>
<li>Word2vec embedding of item on not-wish-to-watch users (10d)</li>
<li>Lsi embedding of user (20d)</li>
<li>Lsi embedding of item (20d)</li>
<li>Lda topic distribution of user on love, like and neutral items (20d)</li>
<li>Lda topic distribution of item on love, like and neutral ratings (20d)</li>
<li>Item categorial (1d, categorial feature)</li>
<li>User Id (1d, only used in FM)</li>
<li>Item Id (1d, only used in FM)</li>
</ul>
<h2 id="Model-ensembing"><a href="#Model-ensembing" class="headerlink" title="Model ensembing"></a>Model ensembing</h2><p>The first layer of stack net is a set of models that should have good capability of prediction but with different inductive bias. Here I just tried three models: GBDT, RF (all backended by <a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="external">lightGBM</a>) and FM (backended by <a href="https://github.com/ibayer/fastFM" target="_blank" rel="external">FastFM</a>). I trained models from record table feature and training table feature separately, and one can further train different models using different combinations of features. For example, one can use all features (except user id and item id) in record table feature. But since GBDT would keep eye on most informative feature if all feature were given, it would be helpful to split features into several groups to train model separately. In this competition, I did not split too much (just because I don’t have too much time). I just removed the first four features (because I see from the prediction result that they have having a major effect on precision) and trained some other models.</p>
<h2 id="Model-stacking"><a href="#Model-stacking" class="headerlink" title="Model stacking"></a>Model stacking</h2><p>The stack net requires one to feed all prediction result from the first layer as feature to second feature. The stacking technique requires one to do KFold cross-validation at the beginning, and then to predict each fold’s result based on all other folds as training data on the second level. Here is the most intuitive (as far as I think) description of model stacking technique: <a href="http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/" target="_blank" rel="external">http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/</a></p>
<p>In this competition, by using a single GBDT and all the features from record table one can reach 0.85567 on LB. By leveraging model stacking technique, one can reach to 0.86155, which is my final score.</p>
<h2 id="Is-this-the-ultimate-ceiling"><a href="#Is-this-the-ultimate-ceiling" class="headerlink" title="Is this the ultimate ceiling?"></a>Is this the ultimate ceiling?</h2><p>Definitely not. One can push the boundary much further:</p>
<ol>
<li>I did not tune the embedding generation parameters very well. In fact, I generated those features using default parameters <a href="https://radimrehurek.com/gensim/" target="_blank" rel="external">gensim</a> provided. The dimension of embeddings are just get by my abrupt decision, no science involved. Maybe one can enlarge the sliding window of word2vec or use more embedding dimensions to achieve better results.</li>
<li>I only used lightGBM to build GBDT. One can also use xgboost. Even though they all provides GBDT, lightGBM is <a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md" target="_blank" rel="external">a leaf-wise tree growth algorithm based model, while xgboost is depth-wise tree growth</a>. Even though two models are all CART based GBDT, they behaves differently.</li>
<li>I did not introduced any deep model generated features. GBDT is such a kind of model that relies on heavy feature engineering while deep model would learn features automatically. By combining them altogether in stacking model one can obtain much higher AUC definitely.</li>
<li>I did not use more complex features. Sometimes, population raking would also effect user’s behavior. A user would select those animes ranked high as “wish to watch”. I did not tried this idea out.</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I must say this competition is very interesting because I see no other competition targets on anime/manga prediction. Another good point of this competition is that the training data is very small, so that I could do CV efficiently on my single workstation. And before this competition, I have never tried stack net before. This competition granted me some experience in how to do model stacking in an engineering experience friendly way.</p>
<p>One thing to regret is that too few competitors were involved in this competition. Though I tried to call for participants to join on <a href="http://chii.in/group/topic/343808" target="_blank" rel="external">Bangumi</a>, it seems still not many people joined. The competition holder should make their website more popular next time before holding next data challenge!</p>
<p>One more thing: one may be interested in the code. I write all my code <a href="https://github.com/wattlebird/MangakiChallenge" target="_blank" rel="external">here</a> but they are not arranged in an organized way. But I think the most important files are: “FeatureExtraction.ipynb” and “aggregation.py”. They are files about how to do feature engineering and how to partition features. “CV.ipynb” gives some intuition on how to train models.</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
		
          <li class="prev disabled"><a><i class="fa fa-arrow-circle-o-left"></i>上一页</a></li>
        

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2017/04/14/Console-as-a-SQL-interface-for-quick-text-file-processing/" class="alignright next">下一页<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        

        
    <!-- JiaThis Button BEGIN -->
    <div class="jiathis_style_24x24">
        <a class="jiathis_button_weixin"></a>
        <a class="jiathis_button_tsina"></a>
        <a class="jiathis_button_twitter"></a>
        <a class="jiathis_button_fb"></a>
        <a class="jiathis_button_googleplus"></a>
        <a class="jiathis_button_linkedin"></a>
        <a class="jiathis_button_copy"></a>
        <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
        <a class="jiathis_counter_style"></a>
    </div>
    <script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
    <!-- JiaThis Button END -->
    <br>


    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">留言</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2017-10-02 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Competition/">Competition<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/Machine-Learning/">Machine Learning<span>5</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
var disqus_shortname = 'wattlebirdgithubio';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2017 Yet another tech blog!
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/latest.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
   </html>
